{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/id.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\", header=None, names=[\"id\"])\n",
    "label_3class = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/label.3class.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\", names=[\"3class_label\"])\n",
    "label_4class = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/label.4class.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\",names=[\"4class_label\"])\n",
    "rating = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/rating.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\",names=[\"rating\"])\n",
    "subj = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\",names=[\"subj_extraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dennis = pd.concat([ids,label_3class, label_4class, rating, subj], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>3class_label</th>\n",
       "      <th>4class_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>subj_extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>in my opinion , a movie reviewer's most import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>you can watch this movie , that is based on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>this is asking a lot to believe , and though i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>no heroes and no story are the main attributes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>this is not an art movie , yet i saw it an art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  3class_label  4class_label  rating  \\\n",
       "0  29420             0             0     0.1   \n",
       "1  17219             0             0     0.2   \n",
       "2  18406             0             0     0.2   \n",
       "3  18648             0             0     0.2   \n",
       "4  20021             0             0     0.2   \n",
       "\n",
       "                                     subj_extraction  \n",
       "0  in my opinion , a movie reviewer's most import...  \n",
       "1  you can watch this movie , that is based on a ...  \n",
       "2  this is asking a lot to believe , and though i...  \n",
       "3  no heroes and no story are the main attributes...  \n",
       "4  this is not an art movie , yet i saw it an art...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dennis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dennis[\"strongly neg\"]=dennis.apply(lambda x:x[\"4class_label\"]==0, axis=1)\n",
    "dennis[\"neg\"]=dennis.apply(lambda x:x[\"4class_label\"]==1, axis=1)\n",
    "dennis[\"pos\"]=dennis.apply(lambda x:x[\"4class_label\"]==2, axis=1)\n",
    "dennis[\"strongly pos\"]=dennis.apply(lambda x:x[\"4class_label\"]==3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = dennis['3class_label']\n",
    "dennis['n']=dennis.apply(lambda x:x[\"4class_label\"]==0, axis=1)\n",
    "dennis[\"neg\"]=dennis.apply(lambda x:x[\"4class_label\"]==1, axis=1)\n",
    "dennis[\"pos\"]=dennis.apply(lambda x:x[\"4class_label\"]==2, axis=1)\n",
    "y_d_encode = dennis[['n', 'neg', 'pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain_d, xvalid_d, ytrain_d, yvalid_d = train_test_split(dennis['subj_extraction'], y_d, \n",
    "                                                  stratify=y_d, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_d, xvalid_d, ytrain_d_encode, yvalid_d_encode = train_test_split(dennis['subj_extraction'], y_d_encode, \n",
    "                                                  stratify=y_d, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          n    neg    pos\n",
       "86     True  False  False\n",
       "124    True  False  False\n",
       "347   False   True  False\n",
       "413   False   True  False\n",
       "630   False  False   True\n",
       "974   False  False  False\n",
       "709   False  False   True\n",
       "807   False  False   True\n",
       "384   False   True  False\n",
       "110    True  False  False\n",
       "557   False   True  False\n",
       "136    True  False  False\n",
       "4      True  False  False\n",
       "913   False  False  False\n",
       "198   False   True  False\n",
       "5      True  False  False\n",
       "371   False   True  False\n",
       "520   False   True  False\n",
       "8      True  False  False\n",
       "564   False   True  False\n",
       "84     True  False  False\n",
       "607   False   True  False\n",
       "238   False   True  False\n",
       "836   False  False   True\n",
       "305   False   True  False\n",
       "60     True  False  False\n",
       "610   False   True  False\n",
       "1010  False  False  False\n",
       "435   False   True  False\n",
       "516   False   True  False\n",
       "...     ...    ...    ...\n",
       "693   False  False   True\n",
       "236   False   True  False\n",
       "427   False   True  False\n",
       "423   False   True  False\n",
       "119    True  False  False\n",
       "736   False  False   True\n",
       "614   False  False   True\n",
       "752   False  False   True\n",
       "11     True  False  False\n",
       "671   False  False   True\n",
       "782   False  False   True\n",
       "368   False   True  False\n",
       "266   False   True  False\n",
       "94     True  False  False\n",
       "137    True  False  False\n",
       "611   False  False   True\n",
       "897   False  False   True\n",
       "449   False   True  False\n",
       "12     True  False  False\n",
       "195   False   True  False\n",
       "512   False   True  False\n",
       "183   False   True  False\n",
       "674   False  False   True\n",
       "323   False   True  False\n",
       "823   False  False   True\n",
       "703   False  False   True\n",
       "615   False  False   True\n",
       "762   False  False   True\n",
       "922   False  False  False\n",
       "664   False  False   True\n",
       "\n",
       "[924 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain_d_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86     it is a call for artistic freedom , but intere...\n",
       "124    steal this movie ! it meant well , showing the...\n",
       "347    a rather flat film noir that never gets that i...\n",
       "413    takeshi \" beat \" kitano is certainly a unique ...\n",
       "630    as the one giving the orders , the big boss , ...\n",
       "Name: subj_extraction, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_d.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain_d) + list(xvalid_d))\n",
    "xtrain_d_tfv =  tfv.transform(xtrain_d) \n",
    "xvalid_d_tfv = tfv.transform(xvalid_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(924, 10512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (xtrain_d.shape)\n",
    "xtrain_d_tfv.shape\n",
    "# for t in xtrain_d_tfv:\n",
    "#     print (t)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.935 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf_l_t = LogisticRegression(C=1.0)\n",
    "clf_l_t.fit(xtrain_d_tfv, ytrain_d)\n",
    "predictions = clf_l_t.predict_proba(xvalid_d_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d_t_l = 0.935"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a simple Naive Bayes on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.938 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf_n_t = MultinomialNB()\n",
    "clf_n_t.fit(xtrain_d_tfv, ytrain_d)\n",
    "predictions = clf_n_t.predict_proba(xvalid_d_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d_t_n = 0.938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting SVM on tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.775 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf_s_t = SVC(probability=True, kernel = 'rbf')\n",
    "clf_s_t.fit(xtrain_d_tfv, ytrain_d)\n",
    "predictions = clf_s_t.predict_proba(xvalid_d_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a three layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924, 10512) (924,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain_d_tfv.shape, ytrain_d.shape)\n",
    "le = LabelEncoder()\n",
    "ytrain_d_encode = le.fit_transform(ytrain_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 10512\n",
    "def model_3_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation = 'relu', input_shape=(num_max,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.summary()\n",
    "#     model.compile(optimizer='rmsprop', \n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "    model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    print ('compile done')\n",
    "    return model\n",
    "\n",
    "def fit_model(model, X, y):\n",
    "    model.fit(X,y,batch_size=32,epochs=10,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               5382656   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,514,241\n",
      "Trainable params: 5,514,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n"
     ]
    }
   ],
   "source": [
    "model = model_3_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 2, 1, 2, 1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain_d_encode[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 739 samples, validate on 185 samples\n",
      "Epoch 1/10\n",
      "739/739 [==============================] - 4s 5ms/step - loss: 0.2264 - acc: 0.4100 - val_loss: -0.3214 - val_acc: 0.4054\n",
      "Epoch 2/10\n",
      "739/739 [==============================] - 2s 3ms/step - loss: -2.0203 - acc: 0.5453 - val_loss: -1.9906 - val_acc: 0.4541\n",
      "Epoch 3/10\n",
      "739/739 [==============================] - 3s 3ms/step - loss: -3.4843 - acc: 0.7429 - val_loss: -2.2054 - val_acc: 0.5243\n",
      "Epoch 4/10\n",
      "739/739 [==============================] - 3s 4ms/step - loss: -3.6420 - acc: 0.7659 - val_loss: -2.2985 - val_acc: 0.5405\n",
      "Epoch 5/10\n",
      "739/739 [==============================] - 3s 4ms/step - loss: -3.6616 - acc: 0.7700 - val_loss: -2.2505 - val_acc: 0.5405\n",
      "Epoch 6/10\n",
      "739/739 [==============================] - 3s 3ms/step - loss: -3.6660 - acc: 0.7700 - val_loss: -2.2363 - val_acc: 0.5459\n",
      "Epoch 7/10\n",
      "739/739 [==============================] - 3s 4ms/step - loss: -3.6673 - acc: 0.7700 - val_loss: -2.2848 - val_acc: 0.5405\n",
      "Epoch 8/10\n",
      "739/739 [==============================] - 3s 4ms/step - loss: -3.6674 - acc: 0.7700 - val_loss: -2.2975 - val_acc: 0.5405\n",
      "Epoch 9/10\n",
      "739/739 [==============================] - 3s 3ms/step - loss: -3.6674 - acc: 0.7700 - val_loss: -2.2782 - val_acc: 0.5405\n",
      "Epoch 10/10\n",
      "739/739 [==============================] - 3s 4ms/step - loss: -3.6674 - acc: 0.7700 - val_loss: -2.2872 - val_acc: 0.5514\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, xtrain_d_tfv, ytrain_d_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain_d) + list(xvalid_d))\n",
    "xtrain_d_ctv =  ctv.transform(xtrain_d) \n",
    "xvalid_d_ctv = ctv.transform(xvalid_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 255418)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_d_ctv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a simple logistic regression on word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.735 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on word Counts\n",
    "clf_l_w = LogisticRegression(C=1.0)\n",
    "clf_l_w.fit(xtrain_d_ctv, ytrain_d)\n",
    "predictions = clf_l_w.predict_proba(xvalid_d_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a simple naive bayes on word count\n",
    "Why do we use NB for text classification?\n",
    "1. the tfidf matrix is sparse, though not independent.\n",
    "2. We can treat tf-idf value as categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 2.655 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on word count\n",
    "clf_n_w = MultinomialNB()\n",
    "clf_n_w.fit(xtrain_d_ctv, ytrain_d)\n",
    "predictions = clf_n_w.predict_proba(xvalid_d_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d_w_n = 2.655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting SVM on word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.973 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on word count\n",
    "clf_s_w = SVC(probability=True, kernel = 'rbf')\n",
    "clf_s_w.fit(xtrain_d_ctv, ytrain_d)\n",
    "predictions = clf_s_w.predict_proba(xvalid_d_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained=True\n",
    "GLOVE_PATH = os.path.dirname(os.getcwd()) + '/models/glove.840B.300d.zip'\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'  # can change it to glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt\n",
    "# load the file\n",
    "if pre_trained:\n",
    "    glove = zipfile.ZipFile(GLOVE_PATH, 'r')\n",
    "    words = pd.read_table(glove.open(GLOVE_FILE), sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "    model = words.as_matrix()\n",
    "else:\n",
    "    model = None\n",
    "# build dictionary\n",
    "if model is not None:\n",
    "    dict1 = {word: i for i, word in enumerate(words.index)}\n",
    "else:\n",
    "    dict1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectorize(s):\n",
    "    words = str(s).lower()\n",
    "#     words = word_tokenize(words)\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "#     words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_d_glv = [sentence_vectorize(sent) for sent in xtrain_d]\n",
    "xvalid_d_glv = [sentence_vectorize(sent) for sent in xvalid_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_d_glv = np.array(xtrain_d_glv)\n",
    "xvalid_d_glv = np.array(xvalid_d_glv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_d_glv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove on Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.072 \n"
     ]
    }
   ],
   "source": [
    "clf_l_g = LogisticRegression(C=1.0)\n",
    "clf_l_g.fit(xtrain_d_glv, ytrain_d)\n",
    "predictions = clf_l_g.predict_proba(xvalid_d_glv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_g_d = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove on Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.072 \n"
     ]
    }
   ],
   "source": [
    "clf_n_g = MultinomialNB()\n",
    "clf_n_g.fit(xtrain_d_glv, ytrain_d)\n",
    "predictions = clf_n_g.predict_proba(xvalid_d_glv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_d, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models above on other writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/id.James+Berardinelli.txt\", sep=\"[\\r\\n]+\", header=None, names=[\"id\"])\n",
    "# label_3class = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/label.3class.James+Berardinelli.txt\", sep=\"[\\r\\n]+\", names=[\"3class_label\"])\n",
    "label_3class = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/label.3class.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"3class_label\"])\n",
    "\n",
    "label_4class = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/label.4class.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"4class_label\"])\n",
    "rating = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/rating.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"rating\"])\n",
    "subj = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"subj_extraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>3class_label</th>\n",
       "      <th>4class_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>subj_extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>i didn't expect ghost in the machine to be thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>admittedly , with a title like the mangler , y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6511</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>cast : christopher lambert , natasha henstridg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>inevitably , someone is going to ask me why i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>ernest : the one-joke concept that refuses to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  3class_label  4class_label  rating  \\\n",
       "0  2321             0             0    0.05   \n",
       "1  3337             0             0    0.05   \n",
       "2  6511             0             0    0.05   \n",
       "3  6912             0             0    0.05   \n",
       "4  2240             0             0    0.09   \n",
       "\n",
       "                                     subj_extraction  \n",
       "0  i didn't expect ghost in the machine to be thi...  \n",
       "1  admittedly , with a title like the mangler , y...  \n",
       "2  cast : christopher lambert , natasha henstridg...  \n",
       "3  inevitably , someone is going to ask me why i ...  \n",
       "4  ernest : the one-joke concept that refuses to ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "james = pd.concat([ids,label_3class, label_4class, rating, subj], axis = 1)\n",
    "james.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_j = james['3class_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_j, xvalid_j, ytrain_j, yvalid_j = train_test_split(james.subj_extraction.values, y_j, \n",
    "                                                  stratify=y_j, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=10512, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain_j) + list(xvalid_j))\n",
    "xtrain_j_tfv =  tfv.transform(xtrain_j) \n",
    "xvalid_j_tfv = tfv.transform(xvalid_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176, 10512)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_j_tfv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a simple Logistic Regression on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.183 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "\n",
    "predictions = clf_l_t.predict_proba(xvalid_j_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a simple Naive Bayes on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.816 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "# clf_n_t = MultinomialNB()\n",
    "clf_n_t.fit(xtrain_j_tfv, ytrain_j)\n",
    "predictions = clf_n_t.predict_proba(xvalid_j_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting SVM on tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.209 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM on TFIDF\n",
    "# clf_s_t = SVC(probability=True, kernel = 'rbf')\n",
    "\n",
    "predictions = clf_s_t.predict_proba(xvalid_j_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a three layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english', max_features = 255418)\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain_j) + list(xvalid_j))\n",
    "xtrain_j_ctv =  ctv.transform(xtrain_j) \n",
    "xvalid_j_ctv = ctv.transform(xvalid_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176, 255418)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_j_ctv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a simple Logistic Regression on word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.278 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on word Counts\n",
    "# clf = LogisticRegression(C=1.0)\n",
    "# clf.fit(xtrain_j_ctv, ytrain_j)\n",
    "predictions = clf_l_w.predict_proba(xvalid_j_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a simple Naive Bayes on wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 10.194 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on wordcount\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(xtrain_j_ctv, ytrain_j)\n",
    "predictions = clf_n_w.predict_proba(xvalid_j_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a svm on wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.448 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf_s_w.predict_proba(xvalid_j_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_j_glv = [sentence_vectorize(sent) for sent in xtrain_j]\n",
    "xvalid_j_glv = [sentence_vectorize(sent) for sent in xvalid_j]\n",
    "xtrain_d_glv = np.array(xtrain_d_glv)\n",
    "xvalid_d_glv = np.array(xvalid_d_glv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Logistic regression on glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.177 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf_l_g.predict_proba(xvalid_j_glv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting NB on glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data for scott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_csv(\"scale_data/scaledata/Scott+Renshaw/id.Scott+Renshaw.txt\", sep=\"[\\r\\n]+\", header=None, names=[\"id\"])\n",
    "label_3class = pd.read_csv(\"scale_data/scaledata/Scott+Renshaw/label.3class.Scott+Renshaw.txt\", sep=\"[\\r\\n]+\", names=[\"3class_label\"])\n",
    "label_4class = pd.read_csv(\"scale_data/scaledata/Scott+Renshaw/label.4class.Scott+Renshaw.txt\", sep=\"[\\r\\n]+\",names=[\"4class_label\"])\n",
    "rating = pd.read_csv(\"scale_data/scaledata/Scott+Renshaw/rating.Scott+Renshaw.txt\", sep=\"[\\r\\n]+\",names=[\"rating\"])\n",
    "subj = pd.read_csv(\"scale_data/scaledata/Scott+Renshaw/subj.Scott+Renshaw.txt\", sep=\"[\\r\\n]+\",names=[\"subj_extraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>3class_label</th>\n",
       "      <th>4class_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>subj_extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11961</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i'm guessing -- and from the available evidenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>there's bad buzz , and then there's the the ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>director : richard rush . director richard rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>screenplay : johnny brennan &amp; kamal ahmed and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>screenplay : tim burns &amp; tom stern and anthony...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  3class_label  4class_label  rating  \\\n",
       "0  11961             0             0     0.0   \n",
       "1  13915             0             0     0.0   \n",
       "2   2790             0             0     0.0   \n",
       "3   3285             0             0     0.0   \n",
       "4  10264             0             0     0.1   \n",
       "\n",
       "                                     subj_extraction  \n",
       "0  i'm guessing -- and from the available evidenc...  \n",
       "1  there's bad buzz , and then there's the the ba...  \n",
       "2  director : richard rush . director richard rus...  \n",
       "3  screenplay : johnny brennan & kamal ahmed and ...  \n",
       "4  screenplay : tim burns & tom stern and anthony...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scott = pd.concat([ids,label_3class, label_4class, rating, subj], axis = 1)\n",
    "scott.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s = james['3class_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_s, xvalid_s, ytrain_s, yvalid_s = train_test_split(james.subj_extraction.values, y_s, \n",
    "                                                  stratify=y_s, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "# tfv = TfidfVectorizer(min_df=3,  max_features=10512, \n",
    "#             strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "#             ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "#             stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain_s) + list(xvalid_s))\n",
    "xtrain_s_tfv =  tfv.transform(xtrain_s) \n",
    "xvalid_s_tfv = tfv.transform(xvalid_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176, 10512)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_s_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.183 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf_l_t.predict_proba(xvalid_s_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_s, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.816 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf_n_t.predict_proba(xvalid_s_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_s, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english', max_features = 10512)\n",
    "\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain_s) + list(xvalid_s))\n",
    "xtrain_s_ctv =  ctv.transform(xtrain_s) \n",
    "xvalid_s_ctv = ctv.transform(xvalid_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 10512 features per sample; expecting 255418",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-d58aedca3da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_l_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_s_tfv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"logloss: %0.3f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmulticlass_logloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myvalid_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 10512 features per sample; expecting 255418"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = clf_l_w.predict_proba(xvalid_s_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_s, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.178 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf_n_g.predict_proba(xvalid_j_glv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid_j, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>log-loss</th>\n",
       "      <th>writer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l_tf</td>\n",
       "      <td>0.935</td>\n",
       "      <td>dennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n_tf</td>\n",
       "      <td>0.938</td>\n",
       "      <td>dennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l_w</td>\n",
       "      <td>0.735</td>\n",
       "      <td>dennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_w</td>\n",
       "      <td>2.655</td>\n",
       "      <td>dennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l_g</td>\n",
       "      <td>1.072</td>\n",
       "      <td>dennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n_g</td>\n",
       "      <td>1.072</td>\n",
       "      <td>dennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l_tf</td>\n",
       "      <td>1.183</td>\n",
       "      <td>James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_tf</td>\n",
       "      <td>0.919</td>\n",
       "      <td>James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l_w</td>\n",
       "      <td>1.278</td>\n",
       "      <td>James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n_w</td>\n",
       "      <td>10.194</td>\n",
       "      <td>James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>l_g</td>\n",
       "      <td>1.177</td>\n",
       "      <td>James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n_g</td>\n",
       "      <td>1.178</td>\n",
       "      <td>James</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classifier  log-loss  writer\n",
       "0        l_tf     0.935  dennis\n",
       "1        n_tf     0.938  dennis\n",
       "2         l_w     0.735  dennis\n",
       "3         n_w     2.655  dennis\n",
       "4         l_g     1.072  dennis\n",
       "5         n_g     1.072  dennis\n",
       "6        l_tf     1.183   James\n",
       "7        n_tf     0.919   James\n",
       "8         l_w     1.278   James\n",
       "9         n_w    10.194   James\n",
       "10        l_g     1.177   James\n",
       "11        n_g     1.178   James"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('comparison_res.csv', delimiter = '\\t')\n",
    "df = df[['classifier', 'log-loss', 'writer']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writer\n",
       "James     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "dennis    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: log-loss, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1164eb588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('writer')['log-loss'].plot(kind = 'line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"classifier\")\n",
    "plt.ylabel('Log loss')\n",
    "x = [0,1,2,3,4,5]\n",
    "y_dennis = df[:6][\"log-loss\"]\n",
    "y_james = df[6:][\"log-loss\"]\n",
    "my_xticks = df[:6][\"classifier\"]\n",
    "plt.xticks(x, my_xticks)\n",
    "plt.plot(x, y_dennis)\n",
    "plt.plot(x, y_james)\n",
    "plt.legend(['Dennis', 'James'])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
