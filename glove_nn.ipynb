{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "# ## Plotly\n",
    "# import plotly.offline as py\n",
    "# import plotly.graph_objs as go\n",
    "# py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/id.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\", header=None, names=[\"id\"])\n",
    "label_3class = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/label.3class.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\", names=[\"3class_label\"])\n",
    "label_4class = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/label.4class.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\",names=[\"4class_label\"])\n",
    "rating = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/rating.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\",names=[\"rating\"])\n",
    "subj = pd.read_csv(\"scale_data/scaledata/Dennis+Schwartz/subj.Dennis+Schwartz.txt\", sep=\"[\\r\\n]+\",names=[\"subj_extraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>3class_label</th>\n",
       "      <th>4class_label</th>\n",
       "      <th>rating</th>\n",
       "      <th>subj_extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>in my opinion , a movie reviewer's most import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>you can watch this movie , that is based on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>this is asking a lot to believe , and though i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>no heroes and no story are the main attributes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>this is not an art movie , yet i saw it an art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  3class_label  4class_label  rating  \\\n",
       "0  29420             0             0     0.1   \n",
       "1  17219             0             0     0.2   \n",
       "2  18406             0             0     0.2   \n",
       "3  18648             0             0     0.2   \n",
       "4  20021             0             0     0.2   \n",
       "\n",
       "                                     subj_extraction  \n",
       "0  in my opinion , a movie reviewer's most import...  \n",
       "1  you can watch this movie , that is based on a ...  \n",
       "2  this is asking a lot to believe , and though i...  \n",
       "3  no heroes and no story are the main attributes...  \n",
       "4  this is not an art movie , yet i saw it an art...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dennis = pd.concat([ids,label_3class, label_4class, rating, subj], axis = 1)\n",
    "dennis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dennis[\"strongly neg\"]=dennis.apply(lambda x:x[\"4class_label\"]==0, axis=1)\n",
    "dennis[\"neg\"]=dennis.apply(lambda x:x[\"4class_label\"]==1, axis=1)\n",
    "dennis[\"pos\"]=dennis.apply(lambda x:x[\"4class_label\"]==2, axis=1)\n",
    "dennis[\"strongly pos\"]=dennis.apply(lambda x:x[\"4class_label\"]==3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = dennis['3class_label']\n",
    "dennis['n']=dennis.apply(lambda x:x[\"4class_label\"]==0, axis=1)\n",
    "dennis[\"neg\"]=dennis.apply(lambda x:x[\"4class_label\"]==1, axis=1)\n",
    "dennis[\"pos\"]=dennis.apply(lambda x:x[\"4class_label\"]==2, axis=1)\n",
    "y_d_encode = dennis[['n', 'neg', 'pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain_d, xvalid_d, ytrain_d, yvalid_d = train_test_split(list(dennis['subj_extraction']), y_d, \n",
    "                                                  stratify=y_d, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_d, xvalid_d, ytrain_d_encode, yvalid_d_encode = train_test_split(dennis['subj_extraction'], y_d_encode, \n",
    "                                                  stratify=y_d, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use x_train as docs, y_d as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(c,f):\n",
    "    return c,f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52343\n",
      ". . . -0.1573 -0.29517 0.30453 -0.54773 0.098293 -0.1776 0.21662 0.19261 -0.21101 0.53788 -0.047755 0.40675 0.023592 -0.32814 0.046858 0.19367 0.25565 -0.021019 -0.15957 -0.1023 0.20303 -0.043333 0.11618 -0.18486 0.0011948 -0.052301 0.34587 0.052335 0.16774 -0.21384 0.055947 0.24934 -0.12179 0.16749 0.28922 -0.033739 0.3015 -0.13241 0.092635 0.37155 -0.2884 -0.0052731 -0.001005 -0.51153 -0.28476 -0.20139 0.11837 -0.0055891 0.43604 0.16796 -0.2701 0.063957 -0.093253 -0.22079 0.36501 0.06545 0.23941 -0.19292 0.098293 0.12172 -0.1168 -0.027436 0.20507 -0.39139 -0.23111 0.46239 0.22888 -0.028415 -0.1798 0.23817 0.28093 -0.47935 0.23177 -0.35587 0.14246 0.11861 0.011018 0.091986 0.0054809 -0.39955 -0.40183 -0.10629 -0.30851 0.12383 -0.16737 -0.43569 0.4211 -0.57416 -0.19964 0.51312 0.090747 -0.21657 0.043519 0.24288 0.081134 0.49104 -0.33342 -0.31056 -0.3136 0.26931 -0.14402 0.33185 -0.21662 -0.072985 0.080603 -0.7266 -0.098385 -0.36233 -0.25346 0.1154 0.25738 0.15802 -0.15633 -0.024581 0.35673 0.31153 0.33475 -0.081155 -0.3061 0.019077 -0.049047 -0.11232 -0.07417 0.35596 -0.2642 0.012781 -0.20715 0.020223 0.054534 -0.28803 0.42863 -0.10312 0.24771 0.013196 0.19768 -0.013528 -0.15134 0.20307 -0.028973 -0.022706 -0.29199 -0.082062 0.19048 0.0053574 0.14067 -0.28675 0.21343 0.42428 -0.28186 -0.11801 -0.45227 -0.0067998 0.044784 -0.0062886 0.25087 0.34481 -0.64459 -0.20467 0.35007 0.1468 -0.14007 -0.0050219 -0.24053 0.41426 -0.40902 0.21141 0.25726 -0.4883 0.027066 0.56367 -0.39594 -0.035206 0.63079 0.14343 0.038315 0.32527 -0.080335 -0.20065 -0.30848 -0.0031591 0.15296 -0.21014 0.42143 -0.20944 -0.069285 0.13555 -0.020401 -0.22555 0.33491 0.16035 0.17739 -0.023627 0.097575 -0.19395 -0.018754 -0.119 -0.0067027 -0.4178 0.29027 0.13034 -0.30212 0.61173 -0.39918 -0.020191 -0.34531 -0.092082 0.46818 0.36671 0.21021 -0.053162 -0.37872 -0.14271 -0.13604 0.31715 -0.17227 -0.091266 0.16417 0.15069 0.53556 -0.29678 0.13965 -0.29788 0.1282 0.1971 -0.045515 -0.41355 -0.050333 -0.39015 -0.29579 -0.096145 -0.03151 0.053714 -0.37309 -0.36523 -0.17235 0.39251 -0.065909 -0.25267 -0.34448 -0.11503 0.43665 0.18832 0.20631 0.27801 -0.046077 0.13397 -0.091953 -0.098542 0.15811 0.2752 0.081383 0.32077 -0.10028 0.1088 -0.24836 0.10477 0.15243 -0.071302 0.12861 0.23061 0.0074864 0.090918 -0.12269 -0.14831 0.010586 0.35745 -0.23412 -0.23746 -0.22646 -0.27641 -0.1634 0.071909 -0.093884 0.21331 -0.20627 0.44406 0.34691 0.019064 0.034657 0.36789 0.32276 -0.31099 -0.023443 -0.77048 -0.26001 0.033961 -0.13874 0.051973 -0.0090509 0.27427 0.046548 -0.48214 -0.1437 -0.1975 -0.038126 -0.16555 0.071697 0.049449 0.15386 -0.81663\n",
      "\n",
      "303\n",
      "151102\n",
      ". . . . . -0.23773 -0.82788 0.82326 -0.91878 0.35868 0.1309 0.26195 -0.30068 0.42963 -1.5335 0.50492 0.59069 0.17763 -0.010302 -0.63371 -0.040832 0.69336 -0.85472 0.693 -0.44824 0.25476 0.17091 0.96056 -0.26516 0.17454 -0.27371 0.20591 -0.60435 -0.50923 0.36049 0.75756 -0.79493 0.12379 0.40019 0.56716 0.19451 0.15996 -0.07882 0.9756 0.54761 -0.25123 -0.70006 -0.78286 -0.95702 0.23045 -0.3643 0.80267 -0.30315 0.35283 -0.19961 0.71134 -0.94116 -0.086543 0.17827 0.40124 -0.071006 0.82202 -0.51055 0.47492 -0.0582 -0.2687 0.94119 0.0066621 -0.53636 -0.68859 0.27149 0.33482 0.53987 -0.72912 -0.53719 0.17591 -0.52643 0.42509 0.12433 -0.17178 0.53168 0.69058 0.703 0.17845 -0.39243 -0.50617 0.39575 0.18461 -0.26065 -0.61119 0.013174 0.32934 -0.4861 -0.46474 0.69677 0.47846 -0.43841 0.051551 0.4195 -0.36847 0.57844 0.053724 -0.026579 -0.5901 0.73221 -0.53987 -0.3428 -0.12343 -0.51213 -0.59237 -0.14589 0.094055 -0.48848 -0.09944 0.092527 0.76764 -0.21207 -0.2395 0.27517 0.27508 0.29604 0.01908 -0.64152 0.51208 0.30948 -0.47126 0.86562 -0.68993 0.19003 -0.78941 -0.10668 -0.64497 0.078153 -0.29598 -0.2383 0.40626 0.066945 0.021937 -0.28291 -0.34962 0.098551 -0.068353 0.06824 -0.17466 -0.21469 1.4612 0.30606 -0.28516 0.078682 0.54627 -0.28879 0.11363 0.32769 -0.38409 -0.038104 -0.70277 -0.47623 -0.29545 0.1906 1.2011 0.38105 0.24595 -0.15847 0.051696 0.71491 0.040783 -0.24121 -0.69558 0.8648 -0.18283 -0.11746 -0.058645 0.033855 -0.32785 -0.23554 -0.12762 0.088837 1.1886 0.6515 -0.32243 0.20434 -0.28283 -0.076 -0.5461 0.66366 -0.19004 0.41687 0.16786 -0.57624 0.0021248 0.038208 0.46581 0.17952 0.15208 0.77239 0.56825 1.3564 0.36707 -0.90959 -0.20369 -0.30854 0.43101 -0.62625 0.62072 -0.48968 -0.4026 0.83121 0.27788 -0.63801 -0.90269 -0.26409 0.55212 -0.21899 0.57153 0.10422 -0.23276 0.32775 0.15975 0.52786 -0.18071 -0.66116 -0.28231 -0.95566 0.32314 -0.10176 0.49961 -0.59512 0.30664 0.17566 -0.0082404 -1.2304 -0.18822 -0.094328 -0.25801 0.07948 0.59872 -0.029741 -0.14526 -0.044699 0.23121 0.20907 -0.442 0.40599 0.16151 -0.49981 0.13384 0.35293 0.034782 0.513 -0.39248 0.39123 0.28351 0.62023 0.6796 0.83006 0.48423 0.70625 -0.25091 -0.23268 0.15136 -0.35641 -0.45169 -0.071751 0.61275 0.12885 -0.62407 0.38831 -0.38358 -0.40596 0.022178 0.44976 -0.11437 -0.43881 -0.72876 -0.76978 -0.42424 0.26061 -0.83878 0.7724 -0.050794 0.0083812 0.18012 -0.77772 -0.22227 0.79349 0.3748 -0.96256 -0.97854 -0.92611 -0.65978 0.080195 -0.25126 1.1857 -0.72262 1.0632 0.50418 0.07075 -0.25384 -0.57426 -0.19791 -0.68991 0.061501 -0.17089 0.18609 -0.78265\n",
      "\n",
      "305\n",
      "209833\n",
      ". . 0.035974 -0.024421 0.71402 -0.61127 0.012771 -0.11201 0.16847 -0.14069 -0.053491 -0.87539 -0.13959 0.29731 0.072308 -0.084514 -0.1879 0.12358 0.37639 -0.39238 -0.01111 -0.04924 0.63649 0.058814 0.19076 -0.20828 -0.11036 0.14934 0.24667 -0.39438 0.22853 -0.11201 0.33539 -0.32929 -0.049727 -0.090764 0.29095 0.27504 0.22802 -0.15616 0.37302 0.3752 -0.3677 0.1518 -0.27551 -0.63281 -0.31298 -0.22441 -0.15435 -0.64802 0.28404 0.12356 0.0034255 0.03094 0.35345 -0.46781 0.59203 -0.17966 0.27702 -0.46738 0.19438 0.21939 -0.36743 -0.084781 0.03253 -0.51323 -0.55466 0.49585 0.066985 0.47906 -0.25118 0.011123 0.15605 -1.0761 0.60875 -0.15764 0.066122 0.12779 -0.089209 0.4311 0.045732 -0.29364 -0.19994 -0.065952 0.26236 0.34039 -0.4956 -0.41187 0.055566 -0.69902 -0.057696 0.76519 0.2018 -0.34497 -0.22707 0.34316 -0.16098 0.42469 0.0080257 -0.33017 -0.43485 0.23581 -0.71085 0.27985 -0.31261 -0.012817 0.48305 -0.75151 -0.02347 -0.39653 -0.86857 0.2877 0.26678 0.22291 -0.1736 -0.12782 0.35032 0.27365 0.28287 0.093409 -0.18104 -0.088499 0.1189 0.026563 -0.027942 0.17254 -0.032427 -0.10745 -0.30334 -0.096047 -0.45369 -0.12113 0.06388 -0.31841 0.0059388 0.17693 -0.21071 -0.6171 0.0018674 0.27296 0.18762 -0.060409 0.5949 0.13994 -0.25773 0.20023 0.4918 0.010659 0.046456 0.4339 -0.10386 0.021517 -0.42845 -0.25458 -0.1307 0.28307 -0.27127 0.080445 -0.31285 -0.12807 0.71382 -0.086474 -0.35553 0.65281 -0.33706 0.38617 -0.12551 0.0056478 0.10091 -0.3638 -0.033486 0.32146 -0.28719 -0.24936 0.65761 0.24376 -0.068703 0.35459 0.080304 -0.30996 -0.20199 0.31482 0.15092 -0.25125 0.76149 -0.33679 -0.079472 -0.04 -0.024693 -0.55248 0.38834 0.14696 0.50003 -0.1377 0.20994 -0.53022 -0.23712 0.24392 0.29524 -0.20951 0.11347 0.16736 -0.057263 0.20945 -0.49785 0.22321 -0.24234 -0.076378 0.42953 0.71138 0.12599 -0.1331 -0.13823 -0.22315 -0.17269 0.43008 -0.34042 -0.23127 0.66599 0.15312 0.47323 -0.28108 0.097872 -0.33014 -0.068678 0.57197 0.099838 -0.6237 -0.22572 -0.59751 -0.30157 -0.1239 0.0057373 -0.058747 -0.030736 -0.10812 0.17601 0.26234 0.15636 -0.19436 -0.097775 0.15462 -0.083865 -0.15106 0.27862 0.28175 -0.27084 0.029867 0.082898 0.020298 0.35015 -0.027691 -0.010642 0.21173 0.090988 0.59747 0.12784 -0.31951 0.26881 -0.41771 -0.2073 -0.077332 -0.32069 -0.020763 -0.24735 -0.23254 -0.005691 -0.088722 -0.13886 -0.16886 -0.13215 -0.17242 -0.1223 -0.24484 0.1374 0.24458 -0.1688 0.64932 0.051973 -0.30896 0.1567 0.351 0.081668 -0.60955 -0.54647 -0.61302 -0.48092 0.22664 -0.27639 0.12523 -0.1358 0.3322 0.54997 -0.068345 0.07199 -0.11543 0.19326 -0.085861 -0.1098 0.034066 -0.072258 -0.58648\n",
      "\n",
      "302\n",
      "220779\n",
      ". . . . 0.033459 -0.085658 0.27155 -0.56132 0.60419 -0.027276 -0.093992 0.068236 -0.3961 -0.83028 0.17456 0.46373 0.13719 0.25598 -0.33885 0.18365 0.44451 -0.8193 -0.081032 -0.070653 0.11253 -0.0087314 0.45494 -0.13481 -0.19651 -0.0098954 0.34683 -0.010663 -0.10555 -0.027425 0.46831 -0.29624 0.0027633 -0.18621 0.32282 0.20276 0.5976 -0.15331 0.52121 0.59813 -0.28581 -0.23798 -0.078883 -0.64561 0.03057 0.28304 0.15654 -0.18034 0.48116 0.39754 0.2106 -0.039421 -0.31166 0.38636 0.64125 -0.52607 0.064417 -0.23567 0.37243 -0.089502 -0.39855 -0.12211 0.37331 -0.45538 -0.40342 0.65258 0.14624 0.3247 -0.41644 0.15981 0.092788 -0.47863 0.64507 -0.013909 0.21356 0.39679 0.52347 0.16871 -0.017134 -0.57287 -0.47366 0.30996 -0.32248 -0.11949 -0.48315 -0.20478 0.45759 -1.0443 -0.58684 0.58544 -0.081284 -0.21224 -0.25302 0.90371 -0.20399 0.65895 -0.11742 -0.13352 -0.091149 0.11375 -0.18618 0.098569 -0.2067 -0.22156 -0.1557 -0.7965 -0.23144 -0.31047 -0.46223 0.26712 0.31644 -0.066401 -0.40895 0.11665 0.50156 0.47769 -0.075403 -0.482 0.033416 0.17506 -0.063225 0.19088 0.039387 0.2396 -0.35331 -0.09274 -0.33705 0.085312 0.080227 -0.020173 0.507 -0.072361 0.13175 -0.45573 0.20334 -0.056897 -0.11733 0.047485 0.016908 -0.24814 0.71598 -0.055958 0.37822 0.3392 0.17434 -0.37932 0.03775 0.019235 -0.29447 -0.18964 -0.45798 -0.16 -0.056206 0.042038 0.64842 0.47592 -0.67641 -0.57816 0.769 -0.096332 -0.064562 0.042617 -0.23167 0.51994 0.10079 0.15415 -0.14515 -0.21889 0.23307 -0.043176 -0.41485 0.1427 0.84153 -0.078821 0.060877 0.12004 -0.26696 -0.53933 -0.36731 -0.37563 -0.35086 -0.24055 0.49705 -0.34699 0.021831 0.53459 0.2906 -0.065185 -0.19822 0.33217 0.28218 0.34624 0.36723 -0.66741 0.082508 -0.10312 0.27671 -0.55848 0.67853 0.040049 -0.0057962 0.86106 -0.40337 -0.45425 -0.48904 -0.11567 0.71084 -0.00071633 -0.075139 0.29584 -0.38594 0.17853 0.033168 0.19186 -0.10717 -0.51614 0.19278 -0.43339 0.51961 0.10363 0.28009 -0.41613 0.34869 0.052315 -0.026509 -0.64101 -0.047879 -0.42739 -0.018592 0.18577 -0.16994 -0.18489 -0.076386 -0.28981 0.26335 0.21274 0.11926 -0.32697 0.22216 -0.24976 0.36953 0.24742 0.37245 0.37218 -0.18512 -0.23093 0.035941 0.2418 0.058993 0.16378 0.43893 0.19057 -0.15457 0.17481 -0.30859 -0.1335 0.21542 -0.44562 0.48477 0.28185 -0.063253 0.053603 -0.05103 -0.29387 0.17704 0.64601 -0.11012 -0.23288 -0.60988 -0.33054 -0.25336 -0.042175 -0.43281 -0.061033 -0.097728 0.42459 0.051169 -0.37042 0.053402 0.18471 0.37567 -0.66422 -0.2099 -0.92126 -0.098194 0.034713 0.19768 0.35687 -0.03349 0.72838 0.1016 -0.42271 -0.19997 -0.21866 -0.2066 -0.59319 -0.052248 -0.032041 0.068443 -0.92476\n",
      "\n",
      "304\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "f = open(os.path.dirname(os.getcwd()) + '/models/glove.840B.300d.txt')\n",
    "count = 0\n",
    "for line in f:\n",
    "    l = line.split()\n",
    "    coe = l[1:]\n",
    "    for c in coe:\n",
    "        if c == '.':\n",
    "            print (count)\n",
    "            print (line)\n",
    "            print (len(l))\n",
    "        break\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,f = x(7,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it is a call for artistic freedom , but interestingly enough , this political film shies away from offering any significant political commentary . a performance artist eats a bar of soap with a knife and fork and is revolted by what he eats , almost as much as the audience is revolted by seeing him go through that routine . this was a hard scene to swallow as not being a contrived one and a very forced metaphor on the political situation , though it provided some much needed levity rather than any particular increase in political insights . the last 20 minutes of the film offers a surprise twist to the story and thereby saves it somewhat from the doldrums it was in . it results in a film that gets its message across that freedom is frozen in china , that things are still far from democratic . it was a though-provoking film , but in a very manipulative way , that was none too pleasing to view despite its good intentions .',\n",
       " 'steal this movie ! it meant well , showing the charismatic abby in the best possible light as the \\'60s prankster symbol of anarchistic , ecstatic , anti-establishment revolution , but this clich ? biopic failed to be emotionally involving , it had no craziness to it . . . and . . . its false sense of piety to its subject ruined the whole point of the film . i read hoffman\\'s steal this book ! the film , adapted from two books -- abbie and anita hoffman\\'s \" to america with love : letters from the underground \" and marty jezer\\'s \" abbie hoffman : american rebel , \" was two goody-goody in its approach to capture either the mood of the changing times or the manic-depressive mood swings of the unabashed publicity hound , who thought of himself as a hippie organizer . admittedly , making a film about the \\'60s ( in this film abbie\\'s starry-eyed days were between 1967-70 ) is not easy to do because of the problem of making a satire about what was a satire to begin with , but this film was so poorly scripted , directed and acted , that it is worst than most films about that era . d\\'onofrio who is a big man does not physically resemble the much smaller hoffman and his odd boston accent sounded artificial . he does bring an energy to the role , but it seems misplaced as he seems to have made the one thing abbie had going for him , a real sense for comedy and wit , seem nonexistent , as the script is so self-serving to him and his lawyer , that it made the whole project stink . the only thing d\\'onofrio does well , is try and steal this movie for himself . all the other roles were flatly played and had no impact on the film .',\n",
       " \"a rather flat film noir that never gets that interesting . it's the simpleness of this tale and lack of any pretensions that makes it convincing but not necessarily sparkling .\",\n",
       " 'takeshi \" beat \" kitano is certainly a unique director , whose absurdist style of filmmaking is readily recognizable , achieving auteur status . he has a fascination with gangsters , violence , and in presenting his quirky black humor in a most compelling way . his films are always visually startling , with a bevy of bizarre shots , and , so far , all his films have gotten away with a mindless but intriguing storyline . he is so fascinating to watch , that it is shame we couldn\\'t see more of his lunacy ; instead , the supporting cast , funny as they are with their deadpan humor , nevertheless , they cannot carry the film , and kitano has chosen to center the film around them . fortunately , there are plenty of his usual visual treats in this film , that make the film seem special anyhow . kitano , also , relishes giving women roles that could be considered demeaning . though it is sickeningly amusing to see how rough-shod he treats them in so callous and an ungentlemanly manner , that one might be caught ruefully laughing at them getting dumped on and treated as garbage , and supposedly liking it and coming back for more , it still shows that he doesn\\'t really know how to portray women on screen . maybe he knows something i don\\'t know . but there is never any romance in his films that amount to anything , and this film is no exception . this mindless act escalates on both sides , and , of course , it makes no sense . this is all casually done , as if this kind of mayhem was as natural as eating rice cakes . if there is any point to this story , and i am not sure there is , but the message here might be , that friends are needed to help you out , no one is immune from the insanity that is in the air , because according to kitano , insanity is catching . and , if taking a stand alone against many , you are in deep trouble . somehow this visual feast of a film was strangely appealing and original in all aspects of filmmaking . so let me say bravo to the one time stand-up comedian who doesn\\'t know how to make a safe film , and tip my eagle baseball hat to him , not only for hitting a home run , but showing that one can hit a home run but not score the needed winning run by running past the man who was on base . a cult film delight as a cruel form of entertainment , that is not for all tastes , naturally . incidently , the american title to the film , boiling point , is misleading : in japanese the title refers in slang to a baseball score .',\n",
       " 'as the one giving the orders , the big boss , he is a less portly version of sydney greenstreet , playing the type of role once reserved for the mirthful greenstreet . don\\'t ask too much more about the plot , it is not particularly pertinent , and some explanations about what this film is about , were obviously snipped off at the cutting room . but an unclear story and poor dubbing quality of most of the non-english speaking actors , does not really hamper the absurdity presented on screen , because it is done in such a way , that it defies any critical commentary , having a dark humor about it that is funnier than hell . chabrol has gone to absurd extremes of humor in this commercially driven screwball comic/mystery , that has the agents working for the greek government , at least i think that is who they are working for . the one in charge of them , is a lazy , sleazy , and mediocre bureaucrat , named sharp ( bouquet ) , aptly named because he is a dull wit , who is clueless about what is going on , as he confronts the two agents working for him , the imaginative but usually wrong , robert , and the more steady dex ( ronet ) , who like sharp , has his eye on robert\\'s appealing wife , shanny , only he doesn\\'t get his face slapped like sharp does by her , as sharp makes too much of an overt pass at her . jean seberg , the much maligned actress , is just terrific in this breezy role , showing a certain amount of vulnerability and courage and sex appeal , and an eye for comic timing , as she takes over for robert when he is killed just before he was going to tell her the info he has about the black boxes , and they were to celebrate the news with champagne and love making . the road to corinthe , the european title for the film , is the road to the marble foundry owned by kalhides . it is the place where there is this great mix of intrigue and murder and comedy , as the black boxes are discovered there , hidden in the statues\\' heads , and as kalhides says , if you think i\\'m not imaginative enough to put them someplace else , you are wrong , because some of them i have put in the base of the statue . it is a wonderful b movie experience , that rips deliciously into the spy film genre and comes away with a bundle of laughs for all its awkwardness and a ridiculous ending to a story that fails to make much sense . i think chabrol hit it best , when the opening epigraph states , \" i do not ask you to believe it , but i suggest that you dream about it . \" well , maybe , you don\\'t want to dream about it , but if you daydream about it during the time of day when you are a little bored with things or upset over something , the absurdity of this film should bring a smile to you .',\n",
       " 'ghost dog is the seventh feature film , of arguably america\\'s most innovative and underrated director , jim jarmusch . i consider it a shame that one of america\\'s best directors had to get european backers for this film and still receives a better reception abroad than he does in his own country . she then says in a low voice , \" you can have it . i\\'m finished with it . \" when louie flinches , he says it is better than if we murder you . there is comedy here in the absurdity of the situation even though no one cracks a joke . but , one of the points of the story , is that things that happen don\\'t necessarily make sense , they just happen . reading books is readily accepted by both parties as something that is cool . the assumption is that they both have soul and can talk the black man\\'s rap . the film is much influenced by jean-pierre melville\\'s stylized 1967 gangster masterpiece le samurai , which also quotes an ancient japanese ideology that influenced its hero , though in that film , it is from the book of bushido , which is a fictional work . ghost dog remains for the most part a parody of other movie characters . the film is slow-paced due to the stoppage to read from the excerpts from the hagakure texts by whitaker . it is a movie that is mostly invigorated by forest whitaker\\'s stunning performance of a man who is an icon , who lives by myth alone , whose most memorable statement , is that \" the end is important in all things . \" the most glaring error ghost dog makes , is in his strictly literal interpretation of what he is supposed to do as a samurai warrior . that would have been the true way of a samurai warrior , whose lessons are meant for battle and seem to be easily misinterpreted by someone living so cut off from human contact and without the benefit of a teacher . jarmusch has in all his films displayed an interest of seeing how another culture looks at the american culture , having already shown how hungarians , native americans , and japanese react to seeing america . it now seems to be the turn of the black man , as this film is basically about the forest whitaker character making peace with who he is , and showing that to be an american , it takes more than one culture to influence you , whether it\\'s the mafia men absorbed by cartoons or rap music or poetry , or the director himself , influenced to be a so-called \" hip \" white negro . for jarmusch , america is an exotic place where different cultures uniquely flourish in the most unexpected spots . the shining points of the film come from the moving cinematography of robby muller , the minimalist mise-en-sc ? ne , the splendid expressive performance by forest whitaker , the underlying humor , and the freshness of jarmusch\\'s approach to filmmaking , proving that he is someone willing to take chances . there are critics who constantly rave about less cutting-edge films more than do about ones that are not afraid to mess with set-concepts . for this film , they offer mostly faint and lefthanded praises . as for me , i\\'ll take my chances with heaping praise on a jarmusch near-miss like this one , over so many other forgettable praiseworthy hits that are so limited in scope . jarmusch\\'s first language is poetry and it is my belief that this film is as close to a lyrical masterpiece as a film can be without being one . its weakest point being a lack of character development for the mafia figures , as the mafia characters were more cartoonish than real , they seemed to be created to fit a certain standard mold rather than to be developed out of their own personalities . but that flaw , was also what drove the film comically , therefore making that weakness more palatable and understandable . this film is similar in mood to his other ambitious work , which was a masterpiece , dead man , and it is one that makes so many fine points to justify its wide range of themes . it is foolish to try and tightly categorize it , except to say that even though it is not exactly a mainstream film , neither is it primarily an art-house film . it just might be one of those intriguing films that requires an acquired taste and once acquired , leaves no doubt about how good it is , no matter that there are a few things in it that might be considered tasteless .',\n",
       " 'kluger is perceived as being tougher and smarter than the d . a . the final shoot-out is well executed and though the story is as plain as vanilla , it still had plenty of tough action . felix e . feist directed this tough b&w noir film in a gritty and pleasing style .',\n",
       " 'fast-paced action and witty one-liners , allow this hitchcock-like story to be both amusing and thrilling . and the surprise ending is a beaut , which should catch you off-guard , as you will most likely be misled and unable to guess who the murderer is . jolly good fun . a real delightful and snappy film , featuring mystery , humor , snappy dialogue , and a great surprise ending .',\n",
       " \"it asks the question , how would you react in finding out that your world is not real ? this is a theme a number of '90s sci-fi films have already done . it seems as if the movies are running out of ideas for this world and need the after-life for more material . supposedly , i am being prepared for a film that will make me think . instead , what i got was a sci-fi film that left me somewhat confused by its being both a murder mystery and a time travel sci-fi'er . it was very difficult to keep track of what was going on , as the film moved back and forth between two different time frames . it was also difficult for me to grok what is meant by the creation of parallel dimensions and how it is possible for our world to be a copy of another . what made it even more difficult , was that the story line was built around the murder case and not around the more interesting ideas generated by time travel . yet , it was this virtual reality computerized world that left me dazzled with its engrossing ideas . though the film tried , it simply couldn't create too much depth for its characters . they remained tied to the sci-fi part of the story . they performed their task well , but their characters couldn't expand because of the limitations they were presented with . they just didn't have too much sparkle to them . even the romance between the leads was jejune . this proved to be a fatal flaw in the film . these creations have feelings and emotions just like people ; but they don't know they're not real . it is too dangerous . yes . the 13th floor , directed by josef rusnak from a screenplay he wrote with ravel centeno-rodriguez can be valued for its re-creation of a 1937 los angeles of fancy bars , booming oil wells , and a magnificent wilshire boulevard setting . all this ostentation taking place in the middle of our great depression . it was fun to see this opulent depiction of a town that was to only grow more opulent and depressed in the future . in this most original story , it seems hannon has made one too many simulations himself and now wants to abort this project . i got mixed messages from this film . i thought it was an interesting concept , that somehow didn't seem as exciting as it should be on the first viewing . i will reserve final judgment on it , since i think it could look better upon seeing it again . as for the flatness of the characters , that i'm afraid won't change . but i must say , i thought the actors did a really fine job considering the limitations placed on them by the script . . . also , the filmmakers did a good job with their use of costume and set designs . i wanted to like this film more than i actually did , but i couldn't fully enjoy it , for the simple reason that too much of the film was taken up by the uninteresting murder case .\",\n",
       " 'this is a pointless b- film actioner , a remake of the gritty 1971 british noir classic directed by mike hodges , which had the good fortune to have michael caine be the star and to have a story with an edge to it . this \" get carter \" features mindless violence , an incoherent and uninvolving story , and most unfortunately the humorless presence of the miscast stallone as a vegas mob enforcer , going to seattle for the funeral of his bartender brother he hasn\\'t seen for a long time . he is dressed in a tacky bright blue suit , sports a goatee , and speaks in a low voice . it is filmed amid some dazzling night shots of las vegas casinos and shots of a rainy seattle , though the rain has no effect on stallone\\'s suit , it never gets soaked no matter how much he exposes it to the elements . it\\'s a clich ? movie . . . so predictable , that if you didn\\'t want to see the way the sleazy bad guys were going to get payback for what they did , you could have left the theater and figured out which way this macho stylish film was going by yourself . you would have missed nothing , there is absolutely nothing interesting about this film . it seems that stephen kay has taken a perfectly taut original film , which was nasty and unremitting to the core , and made a mess of the remake by trying to clean it up , making it into a sleazy film , nevertheless , but one with no redeeming features . he tries to give the film a moment of acting , hugging his niece in a warm embrace , as behind her back he grimaces with obvious pain on his teary face , assuring the audience to pay little attention to this human gesture , they shouldn\\'t worry , he hasn\\'t gone soft , there\\'s going to be a payback . the most disgusting role in the film is reserved for \" gazillionaire \" computer geek alan cumming , he is the simpering coward , the most unmanly and therefore the most unseemly one in the film . it could have been built as the battle between the sleaze champ vs . the egomaniac . it\\'s an undeveloped role , but caine is so good at getting credibility for his character , someone who pretends to have nothing to gain from the murder , who could therefore be honest about what happened , but is , of course , somehow involved in the cover-up . there are several other sleazes around , who have various things to add to the story , such as more gratuitous violence and a seamy look at their lives . since carter fails to heed such warnings , these two will clash in seattle . anyway , no minor b-thriller should be without such a car chase . the only positive spin i could put on this morose work , is to say that it could have been a lot worse , stallone could have talked in a loud voice . this is just another in a long list of films since the 1980s to try and resurrect sylvester stallone\\'s \" comeback . \" this picture won\\'t do . it lacks whatever suspense , character development , and ability to involve the viewer , that is needed . it only features the ego of stallone plodding his way in a shallow story . if you want to see \" get carter , \" rent the 1971 video . it\\'s a nasty story , but it holds together in purpose and credibility , catching hold of the downtrodden newcastle atmosphere in a subtle way , something this film was never able to realize for seattle .']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glove_dl20 as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_matrix, padded_docs = gd.get_glove_m(xtrain_d, ytrain_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [ -3.81940007e-02,  -2.44870007e-01,   7.28120029e-01,\n",
       "         -3.99610013e-01,   8.31720009e-02,   4.39530015e-02,\n",
       "         -3.91409993e-01,   3.34399998e-01,  -5.75450003e-01,\n",
       "          8.74589980e-02,   2.87869990e-01,  -6.73099980e-02,\n",
       "          3.09060007e-01,  -2.63839990e-01,  -1.32310003e-01,\n",
       "         -2.07570001e-01,   3.33950013e-01,  -3.38479996e-01,\n",
       "         -3.17429990e-01,  -4.83359993e-01,   1.46400005e-01,\n",
       "         -3.73039991e-01,   3.45770001e-01,   5.20410016e-02,\n",
       "          4.49460000e-01,  -4.69709992e-01,   2.62800008e-02,\n",
       "         -5.41549981e-01,  -1.55180007e-01,  -1.41069993e-01,\n",
       "         -3.97219993e-02,   2.82770008e-01,   1.43930003e-01,\n",
       "          2.34640002e-01,  -3.10209990e-01,   8.61729980e-02,\n",
       "          2.03970000e-01,   5.26239991e-01,   1.71639994e-01,\n",
       "         -8.23780000e-02,  -7.17869997e-01,  -4.15309995e-01,\n",
       "          2.03349993e-01,  -1.27629995e-01,   4.13670003e-01,\n",
       "          5.51869988e-01,   5.79079986e-01,  -3.34769994e-01,\n",
       "         -3.65590006e-01,  -5.48569977e-01,  -6.28919974e-02,\n",
       "          2.65839994e-01,   3.02049994e-01,   9.97749984e-01,\n",
       "         -8.04809988e-01,  -3.02430010e+00,   1.25399996e-02,\n",
       "         -3.69419992e-01,   2.21670008e+00,   7.22010016e-01,\n",
       "         -2.49779999e-01,   9.21360016e-01,   3.45139988e-02,\n",
       "          4.67449993e-01,   1.10790002e+00,  -1.93580002e-01,\n",
       "         -7.45749995e-02,   2.33530000e-01,  -5.20620011e-02,\n",
       "         -2.20440000e-01,   5.71620017e-02,  -1.58059999e-01,\n",
       "         -3.07980001e-01,  -4.16249990e-01,   3.79720002e-01,\n",
       "          1.50059998e-01,  -5.32119989e-01,  -2.05500007e-01,\n",
       "         -1.25259995e+00,   7.16240034e-02,   7.05649972e-01,\n",
       "          4.97440010e-01,  -4.20630008e-01,   2.61480004e-01,\n",
       "         -1.53799999e+00,  -3.02230000e-01,  -7.34380037e-02,\n",
       "         -2.83120006e-01,   3.71039987e-01,  -2.52169997e-01,\n",
       "          1.62150003e-02,  -1.70990005e-02,  -3.89840007e-01,\n",
       "          8.74239981e-01,  -7.25690007e-01,  -5.10580003e-01,\n",
       "         -5.20280004e-01,  -1.45899996e-01,   8.27799976e-01,\n",
       "          2.70619988e-01],\n",
       "       [ -2.70859987e-01,   4.40060012e-02,  -2.02600006e-02,\n",
       "         -1.73950002e-01,   6.44400001e-01,   7.12130010e-01,\n",
       "          3.55100006e-01,   4.71379995e-01,  -2.96370000e-01,\n",
       "          5.44269979e-01,  -7.22940028e-01,  -4.76119993e-03,\n",
       "          4.06109989e-02,   4.32359986e-02,   2.97289997e-01,\n",
       "          1.07249998e-01,   4.01560009e-01,  -5.36620021e-01,\n",
       "          3.33819985e-02,   6.73960000e-02,   6.45560026e-01,\n",
       "         -8.55230018e-02,   1.41029999e-01,   9.45390016e-02,\n",
       "          7.49469995e-01,  -1.94000006e-01,  -6.87390029e-01,\n",
       "         -4.17409986e-01,  -2.28070006e-01,   1.19999997e-01,\n",
       "         -4.89989996e-01,   8.09449971e-01,   4.51380014e-02,\n",
       "         -1.18979998e-01,   2.01609999e-01,   3.92760009e-01,\n",
       "         -2.01210007e-01,   3.13540012e-01,   7.53040016e-01,\n",
       "          2.59070009e-01,  -1.15659997e-01,  -2.93189995e-02,\n",
       "          9.34989989e-01,  -3.60670000e-01,   5.24200022e-01,\n",
       "          2.37059996e-01,   5.27149975e-01,   2.28689998e-01,\n",
       "         -5.19580007e-01,  -7.93489993e-01,  -2.03679994e-01,\n",
       "         -5.01869977e-01,   1.87480003e-01,   9.42820013e-01,\n",
       "         -4.48339999e-01,  -3.67919993e+00,   4.41830009e-02,\n",
       "         -2.67509997e-01,   2.19970012e+00,   2.40999997e-01,\n",
       "         -3.34249996e-02,   6.95529997e-01,  -6.44720018e-01,\n",
       "         -7.22770020e-03,   8.95749986e-01,   2.00149998e-01,\n",
       "          4.64929998e-01,   6.19329989e-01,  -1.06600001e-01,\n",
       "          8.69100019e-02,  -4.62300003e-01,   1.82620004e-01,\n",
       "         -1.58490002e-01,   2.07909998e-02,   1.93729997e-01,\n",
       "          6.34260029e-02,  -3.16729993e-01,  -4.81770009e-01,\n",
       "         -1.38479996e+00,   1.36690006e-01,   9.68590021e-01,\n",
       "          4.99650016e-02,  -2.73799986e-01,  -3.56860012e-02,\n",
       "         -1.05770004e+00,  -2.44670004e-01,   9.03659999e-01,\n",
       "         -1.24420002e-01,   8.07759985e-02,  -8.34010005e-01,\n",
       "          5.72009981e-01,   8.89450014e-02,  -4.25319999e-01,\n",
       "         -1.82530005e-02,  -7.99949989e-02,  -2.85809994e-01,\n",
       "         -1.08899996e-02,  -4.92300004e-01,   6.36870027e-01,\n",
       "          2.36420006e-01],\n",
       "       [ -7.19529986e-02,   2.31270000e-01,   2.37310007e-02,\n",
       "         -5.06380022e-01,   3.39230001e-01,   1.95899993e-01,\n",
       "         -3.29430014e-01,   1.83640003e-01,  -1.80570006e-01,\n",
       "          2.89629996e-01,   2.04480007e-01,  -5.49600005e-01,\n",
       "          2.73990005e-01,   5.83270013e-01,   2.04679996e-01,\n",
       "         -4.92280006e-01,   1.99739993e-01,  -7.02370033e-02,\n",
       "         -8.80490005e-01,   2.94849992e-01,   1.40709996e-01,\n",
       "         -1.00900002e-01,   9.94490027e-01,   3.69729996e-01,\n",
       "          4.45540011e-01,   2.89979994e-01,  -1.37600005e-01,\n",
       "         -5.63650012e-01,  -2.93649994e-02,  -4.12200004e-01,\n",
       "         -2.52689987e-01,   6.31810009e-01,  -4.47670013e-01,\n",
       "          2.43630007e-01,  -1.08130001e-01,   2.51639992e-01,\n",
       "          4.69669998e-01,   3.75499994e-01,  -2.36129999e-01,\n",
       "         -1.41289994e-01,  -4.45369989e-01,  -6.57369971e-01,\n",
       "         -4.24209982e-02,  -2.86359996e-01,  -2.88109988e-01,\n",
       "          6.37660027e-02,   2.02810004e-01,  -5.35420001e-01,\n",
       "          4.13069993e-01,  -5.97220004e-01,  -3.86139989e-01,\n",
       "          1.93890005e-01,  -1.78090006e-01,   1.66180003e+00,\n",
       "         -1.18190004e-02,  -2.37369990e+00,   5.84269986e-02,\n",
       "         -2.69800007e-01,   1.28230000e+00,   8.19249988e-01,\n",
       "         -2.23220006e-01,   7.29319990e-01,  -5.32109998e-02,\n",
       "          4.35070008e-01,   8.50109994e-01,  -4.29349989e-01,\n",
       "          9.26639974e-01,   3.90509993e-01,   1.05850005e+00,\n",
       "         -2.45609999e-01,  -1.82650000e-01,  -5.32800019e-01,\n",
       "          5.95179982e-02,  -6.60189986e-01,   1.89909995e-01,\n",
       "          2.88360000e-01,  -2.43399993e-01,   5.27840018e-01,\n",
       "         -6.57620013e-01,  -1.40809998e-01,   1.04910004e+00,\n",
       "          5.13400018e-01,  -2.38159999e-01,   6.98949993e-01,\n",
       "         -1.48130000e+00,  -2.48699993e-01,  -1.79360002e-01,\n",
       "         -5.91370016e-02,  -8.05599988e-02,  -4.87819999e-01,\n",
       "          1.44870002e-02,  -6.25899971e-01,  -3.23670000e-01,\n",
       "          4.18619990e-01,  -1.08070004e+00,   4.67420012e-01,\n",
       "         -4.99309987e-01,  -7.18949974e-01,   8.68939996e-01,\n",
       "          1.95390001e-01],\n",
       "       [ -1.89700007e-01,   5.00239991e-02,   1.90840006e-01,\n",
       "         -4.91839983e-02,  -8.97369981e-02,   2.10060000e-01,\n",
       "         -5.49520016e-01,   9.83769968e-02,  -2.01350003e-01,\n",
       "          3.42409998e-01,  -9.26769972e-02,   1.60999998e-01,\n",
       "         -1.32679999e-01,  -2.81599998e-01,   1.87370002e-01,\n",
       "         -4.29589987e-01,   9.60389972e-01,   1.39719993e-01,\n",
       "         -1.07809997e+00,   4.05180007e-01,   5.05389988e-01,\n",
       "         -5.50639987e-01,   4.84400004e-01,   3.80439997e-01,\n",
       "         -2.90549989e-03,  -3.49420011e-01,  -9.96960029e-02,\n",
       "         -7.83680022e-01,   1.03629994e+00,  -2.31399998e-01,\n",
       "         -4.71210003e-01,   5.71259975e-01,  -2.14540005e-01,\n",
       "          3.59580010e-01,  -4.83190000e-01,   1.08749998e+00,\n",
       "          2.85239995e-01,   1.24470003e-01,  -3.92480008e-02,\n",
       "         -7.67320022e-02,  -7.63429999e-01,  -3.24090004e-01,\n",
       "         -5.74899971e-01,  -1.08930004e+00,  -4.18110013e-01,\n",
       "          4.51200008e-01,   1.21119998e-01,  -5.13670027e-01,\n",
       "         -1.33489996e-01,  -1.13779998e+00,  -2.87680000e-01,\n",
       "          1.67740002e-01,   5.58040023e-01,   1.53869998e+00,\n",
       "          1.88590009e-02,  -2.97210002e+00,  -2.42160007e-01,\n",
       "         -9.24950004e-01,   2.19919991e+00,   2.82339990e-01,\n",
       "         -3.47799987e-01,   5.16210020e-01,  -4.33869988e-01,\n",
       "          3.68519992e-01,   7.45729983e-01,   7.21020028e-02,\n",
       "          2.79309988e-01,   9.25689995e-01,  -5.03359996e-02,\n",
       "         -8.58560026e-01,  -1.35800004e-01,  -9.25509989e-01,\n",
       "         -3.39910001e-01,  -1.03939998e+00,  -6.72030002e-02,\n",
       "         -2.13789999e-01,  -4.76900011e-01,   2.13770002e-01,\n",
       "         -8.40080023e-01,   5.25359996e-02,   5.92980027e-01,\n",
       "          2.96039999e-01,  -6.76440001e-01,   1.39160007e-01,\n",
       "         -1.55040002e+00,  -2.07650006e-01,   7.22199976e-01,\n",
       "          5.20560026e-01,  -7.62209967e-02,  -1.51940003e-01,\n",
       "         -1.31339997e-01,   5.86169995e-02,  -3.18690002e-01,\n",
       "         -6.14189982e-01,  -6.23929977e-01,  -4.15479988e-01,\n",
       "         -3.81750017e-02,  -3.98039997e-01,   4.76469994e-01,\n",
       "         -1.59830004e-01],\n",
       "       [ -1.52899995e-01,  -2.42789999e-01,   8.98370028e-01,\n",
       "          1.69960007e-01,   5.35160005e-01,   4.87839997e-01,\n",
       "         -5.88259995e-01,  -1.79820001e-01,  -1.35810006e+00,\n",
       "          4.25410002e-01,   1.53770000e-01,   2.42149994e-01,\n",
       "          1.34739995e-01,   4.11929995e-01,   6.70430005e-01,\n",
       "         -5.64180017e-01,   4.29850012e-01,  -1.21830003e-02,\n",
       "         -1.16769999e-01,   3.17809999e-01,   5.41770011e-02,\n",
       "         -5.42730018e-02,   3.55159998e-01,  -3.02410007e-01,\n",
       "          3.14339995e-01,  -3.38459998e-01,   7.17149973e-01,\n",
       "         -2.68550009e-01,  -1.58370003e-01,  -4.74669993e-01,\n",
       "          5.15809990e-02,  -3.32520008e-01,   1.50030002e-01,\n",
       "         -1.29899994e-01,  -5.46169996e-01,  -3.78430009e-01,\n",
       "          6.42610013e-01,   8.21870029e-01,  -8.00060034e-02,\n",
       "          7.84789994e-02,  -9.69760001e-01,  -5.77409983e-01,\n",
       "          5.64909995e-01,  -3.98730010e-01,  -5.70989996e-02,\n",
       "          1.97430000e-01,   6.57059997e-02,  -4.80919987e-01,\n",
       "         -2.01250002e-01,  -4.08340007e-01,   3.94560009e-01,\n",
       "         -2.64199991e-02,  -1.18380003e-01,   1.01199996e+00,\n",
       "         -5.31710029e-01,  -2.74740005e+00,  -4.29809988e-02,\n",
       "         -7.48489976e-01,   1.75740004e+00,   5.90849996e-01,\n",
       "          4.88499999e-02,   7.82670021e-01,   3.84970009e-01,\n",
       "          4.20969993e-01,   6.78820014e-01,   1.03370003e-01,\n",
       "          6.32799983e-01,  -2.65950002e-02,   5.86470008e-01,\n",
       "         -4.43320006e-01,   3.30570012e-01,  -1.20219998e-01,\n",
       "         -5.56450009e-01,   7.36109987e-02,   2.09150001e-01,\n",
       "          4.33950007e-01,  -1.27609996e-02,   8.98739994e-02,\n",
       "         -1.79910004e+00,   8.48079994e-02,   7.71120012e-01,\n",
       "          6.31049991e-01,  -9.06849980e-01,   6.03259981e-01,\n",
       "         -1.75150001e+00,   1.85959995e-01,  -5.06869972e-01,\n",
       "         -7.02030003e-01,   6.65780008e-01,  -8.13040018e-01,\n",
       "          1.87120005e-01,  -1.84879992e-02,  -2.67569989e-01,\n",
       "          7.26999998e-01,  -5.93630016e-01,  -3.48390013e-01,\n",
       "         -5.60940027e-01,  -5.91000021e-01,   1.00390005e+00,\n",
       "          2.06640005e-01],\n",
       "       [ -5.42639971e-01,   4.14759994e-01,   1.03219998e+00,\n",
       "         -4.02440012e-01,   4.66910005e-01,   2.18160003e-01,\n",
       "         -7.48640001e-02,   4.73320007e-01,   8.09959993e-02,\n",
       "         -2.20789999e-01,  -1.28079996e-01,  -1.14399999e-01,\n",
       "          5.08910000e-01,   1.15680002e-01,   2.82109994e-02,\n",
       "         -3.62800002e-01,   4.38230008e-01,   4.75110002e-02,\n",
       "          2.02820003e-01,   4.98569995e-01,  -1.00680001e-01,\n",
       "          1.32689998e-01,   1.69719994e-01,   1.16530001e-01,\n",
       "          3.13549995e-01,   2.57129997e-01,   9.27829966e-02,\n",
       "         -5.68260014e-01,  -5.29749990e-01,  -5.14560007e-02,\n",
       "         -6.73259974e-01,   9.25329983e-01,   2.69300014e-01,\n",
       "          2.27339998e-01,   6.63649976e-01,   2.62210011e-01,\n",
       "          1.97190002e-01,   2.60899991e-01,   1.87739998e-01,\n",
       "         -3.45400006e-01,  -4.26349998e-01,   1.39750004e-01,\n",
       "          5.63380003e-01,  -5.69069982e-01,   1.23980001e-01,\n",
       "         -1.28940001e-01,   7.24839985e-01,  -2.61049986e-01,\n",
       "         -2.63139993e-01,  -4.36049998e-01,   7.89079964e-02,\n",
       "         -8.41459990e-01,   5.15950024e-01,   1.39970005e+00,\n",
       "         -7.64599979e-01,  -3.14529991e+00,  -2.92019993e-01,\n",
       "         -3.12469989e-01,   1.51289999e+00,   5.24349988e-01,\n",
       "          2.14560002e-01,   4.24519986e-01,  -8.84110034e-02,\n",
       "         -1.78049996e-01,   1.18760002e+00,   1.05789997e-01,\n",
       "          7.65709996e-01,   2.19139993e-01,   3.58240008e-01,\n",
       "         -1.16360001e-01,   9.32610035e-02,  -6.24830008e-01,\n",
       "         -2.18979999e-01,   2.17960000e-01,   7.40559995e-01,\n",
       "         -4.37350005e-01,   1.43429995e-01,   1.47190005e-01,\n",
       "         -1.16050005e+00,  -5.05080000e-02,   1.26770005e-01,\n",
       "         -1.43950004e-02,  -9.86760020e-01,  -9.12970006e-02,\n",
       "         -1.20539999e+00,  -1.19740002e-01,   4.78469990e-02,\n",
       "         -5.40009975e-01,   5.24569988e-01,  -7.09630013e-01,\n",
       "         -3.25280011e-01,  -1.34599999e-01,  -4.13139999e-01,\n",
       "          3.34349990e-01,  -7.24120019e-03,   3.22530001e-01,\n",
       "         -4.42189984e-02,  -1.29690003e+00,   7.62170017e-01,\n",
       "          4.63490009e-01],\n",
       "       [ -3.06639999e-01,   1.68210000e-01,   9.85109985e-01,\n",
       "         -3.36059988e-01,  -2.41600007e-01,   1.61860004e-01,\n",
       "         -5.34959994e-02,   4.30099994e-01,   5.73419988e-01,\n",
       "         -7.15690032e-02,   3.61009985e-01,   2.67289996e-01,\n",
       "          2.77889997e-01,  -7.22680017e-02,   1.38380006e-01,\n",
       "         -2.67140001e-01,   1.29989997e-01,   2.29489997e-01,\n",
       "         -1.83109999e-01,   5.01630008e-01,   4.49209988e-01,\n",
       "         -2.08209995e-02,   4.26420003e-01,  -6.87619969e-02,\n",
       "          4.03369993e-01,   9.51979980e-02,  -3.19440007e-01,\n",
       "         -5.46509981e-01,  -1.33450001e-01,  -5.65110028e-01,\n",
       "         -2.09749997e-01,   1.15919995e+00,  -1.94000006e-01,\n",
       "          1.98280007e-01,  -1.19240001e-01,   4.17809993e-01,\n",
       "          6.83829980e-03,  -2.05369994e-01,  -5.33749998e-01,\n",
       "         -5.22249997e-01,  -3.82270008e-01,  -6.58329995e-03,\n",
       "          1.42649993e-01,  -4.25020009e-01,  -3.11500013e-01,\n",
       "          2.73520011e-03,   7.50930011e-01,  -4.82179999e-01,\n",
       "         -1.85949996e-01,  -7.71040022e-01,  -4.64060009e-02,\n",
       "         -6.91400021e-02,   4.16880012e-01,   1.32350004e+00,\n",
       "         -8.17420006e-01,  -3.39980006e+00,  -1.13070004e-01,\n",
       "         -3.41230005e-01,   2.07750010e+00,   6.13690019e-01,\n",
       "          1.47919998e-01,   9.37529981e-01,  -1.01379998e-01,\n",
       "          2.84260005e-01,   9.78990018e-01,  -3.23350012e-01,\n",
       "          6.36969984e-01,   5.83079994e-01,   2.28200004e-01,\n",
       "         -3.16960007e-01,   2.10610002e-01,  -6.50600016e-01,\n",
       "          2.16529995e-01,  -2.43469998e-01,   5.55190027e-01,\n",
       "         -3.43510002e-01,  -9.50929970e-02,  -1.47149995e-01,\n",
       "         -1.28760004e+00,   3.93099993e-01,   3.01629990e-01,\n",
       "         -2.17669994e-01,  -1.11459994e+00,   5.13490021e-01,\n",
       "         -1.34099996e+00,  -3.03810000e-01,   3.24990004e-01,\n",
       "         -4.52360004e-01,  -1.77430004e-01,  -4.85039987e-02,\n",
       "         -1.21780001e-01,  -4.21079993e-01,  -4.03270006e-01,\n",
       "          3.84519994e-02,  -3.60839993e-01,   3.77379991e-02,\n",
       "         -2.18850002e-01,  -3.87750000e-01,   3.69159997e-01,\n",
       "          5.45210004e-01],\n",
       "       [  1.99159995e-01,  -4.97019999e-02,   2.45790005e-01,\n",
       "         -3.22809994e-01,   8.97679985e-01,  -1.27800003e-01,\n",
       "         -4.95059997e-01,   2.08140001e-01,  -2.00460002e-01,\n",
       "         -2.06039995e-01,   3.82919982e-02,  -6.72770023e-01,\n",
       "         -1.26890004e-01,  -1.87659994e-01,  -1.02770001e-01,\n",
       "          7.31280029e-01,   8.24079990e-01,   8.72879997e-02,\n",
       "          6.92550004e-01,   1.31070006e+00,   4.91129994e-01,\n",
       "         -3.80970001e-01,   2.43379995e-01,  -2.78129995e-01,\n",
       "          6.25060022e-01,   3.59780014e-01,   4.20410007e-01,\n",
       "         -2.45289996e-01,   1.48609996e-01,  -2.67259985e-01,\n",
       "         -5.62619984e-01,   6.38429999e-01,  -5.41530013e-01,\n",
       "          3.65370005e-01,   2.05449998e-01,  -1.66040003e-01,\n",
       "          7.24340022e-01,   2.99609989e-01,  -4.25009996e-01,\n",
       "         -3.59320015e-01,  -8.92879963e-02,   4.87520009e-01,\n",
       "         -1.09270000e+00,   8.88180017e-01,   8.99410009e-01,\n",
       "         -7.54100025e-01,  -3.54920000e-01,  -7.63960004e-01,\n",
       "          2.74679989e-01,   2.75700003e-01,  -4.81519997e-01,\n",
       "         -4.13989991e-01,   6.44890010e-01,   1.14800000e+00,\n",
       "         -2.91310012e-01,  -2.93869996e+00,  -8.31619978e-01,\n",
       "          9.55860019e-01,   1.16229999e+00,  -4.25020009e-01,\n",
       "          1.54860005e-01,   2.23259997e+00,  -3.13389987e-01,\n",
       "         -3.02280001e-02,   7.98020005e-01,  -4.13020015e-01,\n",
       "          7.28850007e-01,   7.29600012e-01,  -3.19090009e-01,\n",
       "          8.95600021e-01,   3.46249998e-01,   2.92299986e-01,\n",
       "          4.00559992e-01,   7.89849997e-01,  -4.39990014e-01,\n",
       "          2.46979997e-01,  -4.65480000e-01,   5.58860004e-02,\n",
       "         -6.26030028e-01,  -3.64869982e-02,  -6.54290020e-01,\n",
       "          1.05630003e-01,   1.74349993e-01,   3.54660004e-01,\n",
       "         -1.94029999e+00,  -2.25019995e-02,  -7.30199993e-01,\n",
       "         -6.30420029e-01,  -3.27990018e-02,  -4.39529985e-01,\n",
       "         -7.23899975e-02,  -4.48749989e-01,  -7.46890008e-02,\n",
       "         -1.44260004e-01,   1.92519993e-01,   2.71079987e-01,\n",
       "          2.03250006e-01,  -6.81089982e-02,   1.76509991e-02,\n",
       "          6.45499974e-02],\n",
       "       [  8.57030004e-02,  -2.22010002e-01,   1.65690005e-01,\n",
       "          1.33729994e-01,   3.82389992e-01,   3.54009986e-01,\n",
       "          1.28699997e-02,   2.24610001e-01,  -4.38169986e-01,\n",
       "          5.01640022e-01,  -3.58740002e-01,  -3.49830002e-01,\n",
       "          5.51560000e-02,   6.96479976e-01,  -1.79580003e-01,\n",
       "          6.79259971e-02,   3.91009986e-01,   1.60390005e-01,\n",
       "         -2.66350001e-01,  -2.11380005e-01,   5.36979973e-01,\n",
       "          4.93790001e-01,   9.36600029e-01,   6.69019997e-01,\n",
       "          2.17930004e-01,  -4.66419995e-01,   2.23830000e-01,\n",
       "         -3.62040013e-01,  -1.76560000e-01,   1.74799994e-01,\n",
       "         -2.03669995e-01,   1.39310002e-01,   1.98320001e-02,\n",
       "         -1.04130000e-01,  -2.02439994e-01,   5.50029993e-01,\n",
       "         -1.54599994e-01,   9.86549973e-01,  -2.68629998e-01,\n",
       "         -2.90899992e-01,  -3.28660011e-01,  -3.41879994e-01,\n",
       "         -1.69430003e-01,  -4.20010000e-01,  -4.67270017e-02,\n",
       "         -1.63269997e-01,   7.08239973e-01,  -7.49109983e-01,\n",
       "         -9.15590003e-02,  -9.61780012e-01,  -1.97469994e-01,\n",
       "          1.02820002e-01,   5.52209973e-01,   1.38160002e+00,\n",
       "         -6.56359971e-01,  -3.25020003e+00,  -3.15560013e-01,\n",
       "         -1.20550001e+00,   1.77090001e+00,   4.02599990e-01,\n",
       "         -7.98269987e-01,   1.15970004e+00,  -3.30419987e-01,\n",
       "          3.13820004e-01,   7.73859978e-01,   2.25950003e-01,\n",
       "          5.24710000e-01,  -3.40530016e-02,   3.20479989e-01,\n",
       "          7.99480006e-02,   1.77520007e-01,  -4.94260013e-01,\n",
       "         -7.00450003e-01,  -4.45690006e-01,   1.72440007e-01,\n",
       "          2.02779993e-01,   2.32919995e-02,  -2.06770003e-01,\n",
       "         -1.01580000e+00,   1.83249995e-01,   5.67520022e-01,\n",
       "          3.18210006e-01,  -6.50110006e-01,   6.82770014e-01,\n",
       "         -8.65849972e-01,  -5.93920015e-02,  -2.92640001e-01,\n",
       "         -5.56680024e-01,  -3.47050011e-01,  -3.28949988e-01,\n",
       "          4.02150005e-01,  -1.27460003e-01,  -2.02280000e-01,\n",
       "          8.73679996e-01,  -5.45000017e-01,   7.92050004e-01,\n",
       "         -2.06949994e-01,  -7.42729977e-02,   7.58080006e-01,\n",
       "         -3.42429996e-01]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
       "         0.82779998,  0.27061999],\n",
       "       [-0.27085999,  0.044006  , -0.02026   , ..., -0.4923    ,\n",
       "         0.63687003,  0.23642001],\n",
       "       ..., \n",
       "       [ 0.49579999, -0.41578001,  0.19389001, ..., -0.30579001,\n",
       "         0.45275   ,  0.76160002],\n",
       "       [-0.46489999, -1.06060004, -0.10805   , ...,  0.036301  ,\n",
       "        -0.85459   ,  0.29347   ],\n",
       "       [-0.032544  ,  0.093303  ,  0.0076568 , ...,  0.63237   ,\n",
       "         0.43331   ,  0.84724998]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded_docs = gd.padded_doc(xvalid_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 4)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix.shape\n",
    "vocab_size\n",
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 4, 100)            1791100   \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,791,501\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,791,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "924/924 [==============================] - 1s 1ms/step - loss: 0.6160 - acc: 0.4026\n",
      "Epoch 2/15\n",
      "924/924 [==============================] - 0s 70us/step - loss: 0.3801 - acc: 0.4134\n",
      "Epoch 3/15\n",
      "924/924 [==============================] - 0s 76us/step - loss: 0.3469 - acc: 0.4123\n",
      "Epoch 4/15\n",
      "924/924 [==============================] - 0s 85us/step - loss: 0.3228 - acc: 0.4145\n",
      "Epoch 5/15\n",
      "924/924 [==============================] - 0s 67us/step - loss: 0.3032 - acc: 0.4145\n",
      "Epoch 6/15\n",
      "924/924 [==============================] - 0s 64us/step - loss: 0.2814 - acc: 0.4188\n",
      "Epoch 7/15\n",
      "924/924 [==============================] - 0s 75us/step - loss: 0.2639 - acc: 0.4188\n",
      "Epoch 8/15\n",
      "924/924 [==============================] - ETA: 0s - loss: 0.2403 - acc: 0.420 - 0s 73us/step - loss: 0.2419 - acc: 0.4188\n",
      "Epoch 9/15\n",
      "924/924 [==============================] - 0s 86us/step - loss: 0.2249 - acc: 0.4188\n",
      "Epoch 10/15\n",
      "924/924 [==============================] - 0s 62us/step - loss: 0.2081 - acc: 0.4188\n",
      "Epoch 11/15\n",
      "924/924 [==============================] - 0s 72us/step - loss: 0.1896 - acc: 0.4210\n",
      "Epoch 12/15\n",
      "924/924 [==============================] - 0s 73us/step - loss: 0.1721 - acc: 0.4210\n",
      "Epoch 13/15\n",
      "924/924 [==============================] - 0s 66us/step - loss: 0.1575 - acc: 0.4210\n",
      "Epoch 14/15\n",
      "924/924 [==============================] - 0s 95us/step - loss: 0.1443 - acc: 0.4232\n",
      "Epoch 15/15\n",
      "924/924 [==============================] - 0s 94us/step - loss: 0.1281 - acc: 0.4242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1381a4518>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, ytrain_d, epochs=15, verbose=1)\n",
    "\n",
    "# # evaluate the model\n",
    "# loss, accuracy = model.evaluate(x_test_padded_docs, yvalid_d, verbose=0)\n",
    "# print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 2 from 1 for 'max_pooling1d_26/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling1d_26/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-27003b29019a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                          \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                          strides=1)(z)\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mconv_blocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                                         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                                         data_format='channels_last')\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove dummy last dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/layers/pooling.py\u001b[0m in \u001b[0;36m_pooling_function\u001b[0;34m(self, inputs, pool_size, strides, padding, data_format)\u001b[0m\n\u001b[1;32m     81\u001b[0m                           padding, data_format):\n\u001b[1;32m     82\u001b[0m         output = K.pool2d(inputs, pool_size, strides,\n\u001b[0;32m---> 83\u001b[0;31m                           padding, data_format, pool_mode='max')\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mpool2d\u001b[0;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         x = tf.nn.max_pool(x, pool_size, strides,\n\u001b[1;32m   3653\u001b[0m                            \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m                            data_format=tf_data_format)\n\u001b[0m\u001b[1;32m   3655\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         x = tf.nn.avg_pool(x, pool_size, strides,\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   2041\u001b[0m                                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m                                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m_max_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   3016\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   3017\u001b[0m         \u001b[0;34m\"MaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3018\u001b[0;31m         data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   3019\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         op_def=op_def)\n\u001b[1;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3162\u001b[0;31m                            compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;31m# compute_shapes argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling1d_26/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,10]."
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "# define model\n",
    "embedding_dim = 50\n",
    "filter_sizes = (1, 4)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Build model\n",
    "sequence_length = 4\n",
    "input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "# Static model does not have embedding layer\n",
    "\n",
    "z = Embedding(vocab_size, 100, input_length=sequence_length,weights=[embedding_matrix], name=\"embedding\")(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # Initialize weights with word2vec\n",
    "# if model_type == \"CNN-non-static\":\n",
    "#     weights = np.array([v for v in embedding_weights.values()])\n",
    "#     print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "#     embedding_layer = model.get_layer(\"embedding\")\n",
    "#     embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_docs, ytrain_d, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 4, 100)            1791100   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 4, 60)             38640     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,832,841\n",
      "Trainable params: 41,741\n",
      "Non-trainable params: 1,791,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "924/924 [==============================] - 3s 3ms/step - loss: 0.5099 - acc: 0.4113\n",
      "Epoch 2/50\n",
      "924/924 [==============================] - 0s 407us/step - loss: 0.3823 - acc: 0.4156\n",
      "Epoch 3/50\n",
      "924/924 [==============================] - 0s 410us/step - loss: 0.3426 - acc: 0.4156\n",
      "Epoch 4/50\n",
      "924/924 [==============================] - 0s 457us/step - loss: 0.3002 - acc: 0.4156\n",
      "Epoch 5/50\n",
      "924/924 [==============================] - 0s 435us/step - loss: 0.2749 - acc: 0.4167\n",
      "Epoch 6/50\n",
      "924/924 [==============================] - 0s 411us/step - loss: 0.2185 - acc: 0.4177\n",
      "Epoch 7/50\n",
      "924/924 [==============================] - 0s 417us/step - loss: 0.1170 - acc: 0.4286\n",
      "Epoch 8/50\n",
      "924/924 [==============================] - 0s 413us/step - loss: -0.0288 - acc: 0.4307\n",
      "Epoch 9/50\n",
      "924/924 [==============================] - 0s 447us/step - loss: -0.2122 - acc: 0.4535\n",
      "Epoch 10/50\n",
      "924/924 [==============================] - 0s 417us/step - loss: -0.4580 - acc: 0.4437\n",
      "Epoch 11/50\n",
      "924/924 [==============================] - 0s 407us/step - loss: -0.5103 - acc: 0.4426\n",
      "Epoch 12/50\n",
      "924/924 [==============================] - 0s 437us/step - loss: -0.6998 - acc: 0.4675\n",
      "Epoch 13/50\n",
      "924/924 [==============================] - 0s 450us/step - loss: -0.8009 - acc: 0.4697\n",
      "Epoch 14/50\n",
      "924/924 [==============================] - 0s 417us/step - loss: -0.9721 - acc: 0.4675\n",
      "Epoch 15/50\n",
      "924/924 [==============================] - 0s 459us/step - loss: -1.0345 - acc: 0.4686\n",
      "Epoch 16/50\n",
      "924/924 [==============================] - 0s 388us/step - loss: -1.1403 - acc: 0.4859\n",
      "Epoch 17/50\n",
      "924/924 [==============================] - 0s 432us/step - loss: -1.2852 - acc: 0.4978\n",
      "Epoch 18/50\n",
      "924/924 [==============================] - 0s 418us/step - loss: -1.3268 - acc: 0.4859\n",
      "Epoch 19/50\n",
      "924/924 [==============================] - 0s 415us/step - loss: -1.3318 - acc: 0.4989\n",
      "Epoch 20/50\n",
      "924/924 [==============================] - 0s 427us/step - loss: -1.4547 - acc: 0.5000\n",
      "Epoch 21/50\n",
      "924/924 [==============================] - 0s 411us/step - loss: -1.6309 - acc: 0.5195\n",
      "Epoch 22/50\n",
      "924/924 [==============================] - 0s 403us/step - loss: -1.7255 - acc: 0.5216\n",
      "Epoch 23/50\n",
      "924/924 [==============================] - 0s 410us/step - loss: -1.7703 - acc: 0.5335\n",
      "Epoch 24/50\n",
      "924/924 [==============================] - 0s 416us/step - loss: -1.7480 - acc: 0.5281\n",
      "Epoch 25/50\n",
      "924/924 [==============================] - 0s 402us/step - loss: -1.9680 - acc: 0.5206\n",
      "Epoch 26/50\n",
      "924/924 [==============================] - 0s 403us/step - loss: -2.0581 - acc: 0.5400\n",
      "Epoch 27/50\n",
      "924/924 [==============================] - 0s 430us/step - loss: -1.9920 - acc: 0.5368\n",
      "Epoch 28/50\n",
      "924/924 [==============================] - 0s 439us/step - loss: -2.0872 - acc: 0.5400\n",
      "Epoch 29/50\n",
      "924/924 [==============================] - 0s 406us/step - loss: -2.2414 - acc: 0.5563\n",
      "Epoch 30/50\n",
      "924/924 [==============================] - 0s 427us/step - loss: -2.3933 - acc: 0.5649\n",
      "Epoch 31/50\n",
      "924/924 [==============================] - 0s 437us/step - loss: -2.3511 - acc: 0.5606\n",
      "Epoch 32/50\n",
      "924/924 [==============================] - 0s 435us/step - loss: -2.4007 - acc: 0.5628\n",
      "Epoch 33/50\n",
      "924/924 [==============================] - 0s 397us/step - loss: -2.5438 - acc: 0.5985\n",
      "Epoch 34/50\n",
      "924/924 [==============================] - 0s 351us/step - loss: -2.5344 - acc: 0.5985\n",
      "Epoch 35/50\n",
      "924/924 [==============================] - 0s 402us/step - loss: -2.6199 - acc: 0.5942\n",
      "Epoch 36/50\n",
      "924/924 [==============================] - 0s 426us/step - loss: -2.6682 - acc: 0.5920\n",
      "Epoch 37/50\n",
      "924/924 [==============================] - 0s 449us/step - loss: -2.6944 - acc: 0.5823\n",
      "Epoch 38/50\n",
      "924/924 [==============================] - 0s 356us/step - loss: -2.7497 - acc: 0.5855\n",
      "Epoch 39/50\n",
      "924/924 [==============================] - 0s 414us/step - loss: -2.8366 - acc: 0.6147\n",
      "Epoch 40/50\n",
      "924/924 [==============================] - 0s 413us/step - loss: -2.8578 - acc: 0.6299\n",
      "Epoch 41/50\n",
      "924/924 [==============================] - 0s 406us/step - loss: -2.7756 - acc: 0.6158\n",
      "Epoch 42/50\n",
      "924/924 [==============================] - 0s 445us/step - loss: -2.9687 - acc: 0.6136\n",
      "Epoch 43/50\n",
      "924/924 [==============================] - 0s 388us/step - loss: -2.9737 - acc: 0.6310\n",
      "Epoch 44/50\n",
      "924/924 [==============================] - 0s 388us/step - loss: -3.0331 - acc: 0.6569\n",
      "Epoch 45/50\n",
      "924/924 [==============================] - 0s 394us/step - loss: -2.9751 - acc: 0.6288\n",
      "Epoch 46/50\n",
      "924/924 [==============================] - 0s 408us/step - loss: -3.0053 - acc: 0.6331\n",
      "Epoch 47/50\n",
      "924/924 [==============================] - 0s 402us/step - loss: -3.0667 - acc: 0.6558\n",
      "Epoch 48/50\n",
      "924/924 [==============================] - 0s 426us/step - loss: -3.0881 - acc: 0.6645\n",
      "Epoch 49/50\n",
      "924/924 [==============================] - 0s 439us/step - loss: -3.1089 - acc: 0.6461\n",
      "Epoch 50/50\n",
      "924/924 [==============================] - 0s 462us/step - loss: -3.1291 - acc: 0.6591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ea94ac8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "# model.add(Flatten())\n",
    "model.add(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, ytrain_d, epochs=50, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-18c7c1b025ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fit_model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.660194\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test_padded_docs, yvalid_d, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_d_glv = np.array(xtrain_d_glv)\n",
    "xvalid_d_glv = np.array(xvalid_d_glv)\n",
    "xtrain_d_glv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 300\n",
    "def model_3_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation = 'relu', input_shape=(num_max,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation = 'sigmoid'))\n",
    "    model.summary()\n",
    "#     model.compile(optimizer='rmsprop', \n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "    model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    print ('compile done')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X, y):\n",
    "    model.fit(X,y,batch_size=32,epochs=10,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 286,211\n",
      "Trainable params: 286,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n"
     ]
    }
   ],
   "source": [
    "model = model_3_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 739 samples, validate on 185 samples\n",
      "Epoch 1/10\n",
      "739/739 [==============================] - 1s 1ms/step - loss: 0.6900 - acc: 0.6978 - val_loss: 0.6874 - val_acc: 0.7027\n",
      "Epoch 2/10\n",
      "739/739 [==============================] - 0s 276us/step - loss: 0.6854 - acc: 0.7032 - val_loss: 0.6833 - val_acc: 0.7027\n",
      "Epoch 3/10\n",
      "739/739 [==============================] - 0s 239us/step - loss: 0.6813 - acc: 0.7032 - val_loss: 0.6792 - val_acc: 0.7027\n",
      "Epoch 4/10\n",
      "739/739 [==============================] - 0s 269us/step - loss: 0.6773 - acc: 0.7032 - val_loss: 0.6753 - val_acc: 0.7027\n",
      "Epoch 5/10\n",
      "739/739 [==============================] - ETA: 0s - loss: 0.6735 - acc: 0.700 - 0s 282us/step - loss: 0.6734 - acc: 0.7032 - val_loss: 0.6715 - val_acc: 0.7027\n",
      "Epoch 6/10\n",
      "739/739 [==============================] - 0s 274us/step - loss: 0.6695 - acc: 0.7032 - val_loss: 0.6678 - val_acc: 0.7027\n",
      "Epoch 7/10\n",
      "739/739 [==============================] - 0s 356us/step - loss: 0.6658 - acc: 0.7032 - val_loss: 0.6642 - val_acc: 0.7027\n",
      "Epoch 8/10\n",
      "739/739 [==============================] - 0s 379us/step - loss: 0.6623 - acc: 0.7032 - val_loss: 0.6609 - val_acc: 0.7027\n",
      "Epoch 9/10\n",
      "739/739 [==============================] - 0s 296us/step - loss: 0.6589 - acc: 0.7032 - val_loss: 0.6575 - val_acc: 0.7027\n",
      "Epoch 10/10\n",
      "739/739 [==============================] - 0s 292us/step - loss: 0.6556 - acc: 0.7032 - val_loss: 0.6543 - val_acc: 0.7027\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_gl.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_gl.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove_nn on the other writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/id.James+Berardinelli.txt\", sep=\"[\\r\\n]+\", header=None, names=[\"id\"])\n",
    "# label_3class = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/label.3class.James+Berardinelli.txt\", sep=\"[\\r\\n]+\", names=[\"3class_label\"])\n",
    "label_3class = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/label.3class.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"3class_label\"])\n",
    "\n",
    "label_4class = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/label.4class.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"4class_label\"])\n",
    "rating = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/rating.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"rating\"])\n",
    "subj = pd.read_csv(\"scale_data/scaledata/James+Berardinelli/subj.James+Berardinelli.txt\", sep=\"[\\r\\n]+\",names=[\"subj_extraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "james = pd.concat([ids,label_3class, label_4class, rating, subj], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "james[\"strongly neg\"]=james.apply(lambda x:x[\"4class_label\"]==0, axis=1)\n",
    "james[\"neg\"]=james.apply(lambda x:x[\"4class_label\"]==1, axis=1)\n",
    "james[\"pos\"]=james.apply(lambda x:x[\"4class_label\"]==2, axis=1)\n",
    "james[\"strongly pos\"]=james.apply(lambda x:x[\"4class_label\"]==3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = dennis['3class_label']\n",
    "james['n']=james.apply(lambda x:x[\"4class_label\"]==0, axis=1)\n",
    "james[\"neg\"]=james.apply(lambda x:x[\"4class_label\"]==1, axis=1)\n",
    "james[\"pos\"]=james.apply(lambda x:x[\"4class_label\"]==2, axis=1)\n",
    "y_j_encode = james[['n', 'neg', 'pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_j = james.subj_extraction.values\n",
    "y_j = james['3class_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain_j, xvalid_j, ytrain_j, yvalid_j = train_test_split(x_j, y_j, \n",
    "                                                  stratify=y_j, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_j, xvalid_j, ytrain_j_encode, yvalid_j_encode = train_test_split(x_j, y_j_encode, \n",
    "                                                  stratify=y_j, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176, 300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_j_glv = [g.sentence_vectorize(sent) for sent in xtrain_j]\n",
    "xvalid_j_glv = [g.sentence_vectorize(sent) for sent in xvalid_j]\n",
    "xtrain_j_glv = np.array(xtrain_j_glv)\n",
    "xvalid_j_glv = np.array(xvalid_j_glv)\n",
    "\n",
    "xtrain_j_glv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the pre_trained model and evaluate on james directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "acc: 73.79%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('model_gl.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_gl.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(xvalid_j_glv, yvalid_j_encode, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit 3 layer-nn on james"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 286,211\n",
      "Trainable params: 286,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n"
     ]
    }
   ],
   "source": [
    "model = model_3_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 940 samples, validate on 236 samples\n",
      "Epoch 1/10\n",
      "940/940 [==============================] - 0s 295us/step - loss: 0.6486 - acc: 0.7379 - val_loss: 0.6466 - val_acc: 0.7401\n",
      "Epoch 2/10\n",
      "940/940 [==============================] - 0s 283us/step - loss: 0.6430 - acc: 0.7379 - val_loss: 0.6408 - val_acc: 0.7401\n",
      "Epoch 3/10\n",
      "940/940 [==============================] - 0s 266us/step - loss: 0.6378 - acc: 0.7379 - val_loss: 0.6351 - val_acc: 0.7401\n",
      "Epoch 4/10\n",
      "940/940 [==============================] - 0s 290us/step - loss: 0.6328 - acc: 0.7379 - val_loss: 0.6298 - val_acc: 0.7401\n",
      "Epoch 5/10\n",
      "940/940 [==============================] - 0s 275us/step - loss: 0.6280 - acc: 0.7379 - val_loss: 0.6245 - val_acc: 0.7401\n",
      "Epoch 6/10\n",
      "940/940 [==============================] - 0s 284us/step - loss: 0.6232 - acc: 0.7379 - val_loss: 0.6192 - val_acc: 0.7401\n",
      "Epoch 7/10\n",
      "940/940 [==============================] - 0s 293us/step - loss: 0.6185 - acc: 0.7379 - val_loss: 0.6143 - val_acc: 0.7401\n",
      "Epoch 8/10\n",
      "940/940 [==============================] - 0s 292us/step - loss: 0.6140 - acc: 0.7379 - val_loss: 0.6094 - val_acc: 0.7401\n",
      "Epoch 9/10\n",
      "940/940 [==============================] - 0s 327us/step - loss: 0.6097 - acc: 0.7379 - val_loss: 0.6047 - val_acc: 0.7401\n",
      "Epoch 10/10\n",
      "940/940 [==============================] - 0s 261us/step - loss: 0.6055 - acc: 0.7379 - val_loss: 0.6001 - val_acc: 0.7401\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, xtrain_j_glv, ytrain_j_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x12ca334e0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 940 samples, validate on 236 samples\n",
      "Epoch 1/10\n",
      "940/940 [==============================] - 1s 1ms/step - loss: 0.6473 - acc: 0.7379 - val_loss: 0.6486 - val_acc: 0.7401\n",
      "Epoch 2/10\n",
      "940/940 [==============================] - 0s 244us/step - loss: 0.6468 - acc: 0.7379 - val_loss: 0.6480 - val_acc: 0.7401\n",
      "Epoch 3/10\n",
      "940/940 [==============================] - 0s 274us/step - loss: 0.6463 - acc: 0.7379 - val_loss: 0.6474 - val_acc: 0.7401\n",
      "Epoch 4/10\n",
      "940/940 [==============================] - 0s 268us/step - loss: 0.6457 - acc: 0.7379 - val_loss: 0.6469 - val_acc: 0.7401\n",
      "Epoch 5/10\n",
      "940/940 [==============================] - 0s 294us/step - loss: 0.6452 - acc: 0.7379 - val_loss: 0.6463 - val_acc: 0.7401\n",
      "Epoch 6/10\n",
      "940/940 [==============================] - 0s 281us/step - loss: 0.6446 - acc: 0.7379 - val_loss: 0.6457 - val_acc: 0.7401\n",
      "Epoch 7/10\n",
      "940/940 [==============================] - 0s 261us/step - loss: 0.6441 - acc: 0.7379 - val_loss: 0.6452 - val_acc: 0.7401\n",
      "Epoch 8/10\n",
      "940/940 [==============================] - 0s 284us/step - loss: 0.6436 - acc: 0.7379 - val_loss: 0.6446 - val_acc: 0.7401\n",
      "Epoch 9/10\n",
      "940/940 [==============================] - 0s 277us/step - loss: 0.6431 - acc: 0.7379 - val_loss: 0.6440 - val_acc: 0.7401\n",
      "Epoch 10/10\n",
      "940/940 [==============================] - 0s 290us/step - loss: 0.6426 - acc: 0.7379 - val_loss: 0.6435 - val_acc: 0.7401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1292176a0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.fit(xtrain_j_glv, ytrain_j_encode, batch_size=32,epochs=10,verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn on glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128,5,\n",
    "                 activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Conv2D(64, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 300\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 100\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,\n",
    "                    400,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(xtrain_j_glv, ytrain_j_encode,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(xvalid_j_glv, yvalid_j_encode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
